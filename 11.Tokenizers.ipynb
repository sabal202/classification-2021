{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация\n",
    "\n",
    "Задача - разделить предложение на слова или отдельные элементы (знаки препинания, гиперссылки и т.д.), по возможности сохраняя какие-то атрибуты текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регурярные выражения\n",
    "\n",
    "В модуле `re` есть недокументированный класс `Scanner`, с помощью которого можно реализовать лексический анализатор. `Scanner` будет искать вхождения паттернов в тексте и на каждое совпадение вызывать соответствующую функцию. В общем случае подобный код неэффективен, лексические анализаторы лучше реализовывать с помощью специальных инструментов - генераторов лексических анализаторов, которые обеспечат анализ за линейное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('hello', 'word'),\n",
       "  (',', 'preposition'),\n",
       "  (' ', 'whitespace'),\n",
       "  ('world', 'word'),\n",
       "  (' ', 'whitespace'),\n",
       "  ('1234', 'digit'),\n",
       "  (' ', 'whitespace'),\n",
       "  ('test@example.com', 'email')],\n",
       " '')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "scanner = re.Scanner(\n",
    "   [(r'(\\w+)@(\\w+)\\.(\\w{2,3})', lambda s, x: (x, 'email')),\n",
    "    (r'[a-zA-Z]+', lambda s, x: (x, 'word')), \n",
    "    (r'\\d+', lambda s, x: (x, 'digit')),    \n",
    "    (r'\\s+', lambda s, x: (x, 'whitespace')),\n",
    "    (r'[.,;\"!?:]', lambda s, x: (x, 'preposition')),\n",
    "    ])\n",
    "\n",
    "scanner.scan('hello, world 1234 test@example.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "Natural Language Toolkit, библиотека для обработки естественных языков. Она создавалась для учебных целей, но тем не менее приобрела определенную популярность. Реализовано некоторое количество методов токенизации, которые можно использовать для повседневных задач и экспериментов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Hello world 4.2.\n",
      "word_tokenize:  ['Hello', 'world', '4.2', '.']\n",
      "wordpunct_tokenize:  ['Hello', 'world', '4', '.', '2', '.']\n",
      "tweet:  ['Hello', 'world', '4.2', '.']\n",
      "\n",
      "Sentence: LA New-York\n",
      "word_tokenize:  ['LA', 'New-York']\n",
      "wordpunct_tokenize:  ['LA', 'New', '-', 'York']\n",
      "tweet:  ['LA', 'New-York']\n",
      "\n",
      "Sentence: Hello world 4.2!\n",
      "word_tokenize:  ['Hello', 'world', '4.2', '!']\n",
      "wordpunct_tokenize:  ['Hello', 'world', '4', '.', '2', '!']\n",
      "tweet:  ['Hello', 'world', '4.2', '!']\n",
      "\n",
      "Sentence: Say me #hello\n",
      "word_tokenize:  ['Say', 'me', '#', 'hello']\n",
      "wordpunct_tokenize:  ['Say', 'me', '#', 'hello']\n",
      "tweet:  ['Say', 'me', '#hello']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize, word_tokenize, TweetTokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "tweet_tokenize = TweetTokenizer()\n",
    "\n",
    "sentences = [\"Hello world 4.2.\", \"LA New-York\", \"Hello world 4.2!\", \"Say me #hello\"]\n",
    "\n",
    "for sent in sentences:\n",
    "    print(\"Sentence: {}\".format(sent))\n",
    "    print(\"word_tokenize: \", word_tokenize(sent))\n",
    "    print(\"wordpunct_tokenize: \", wordpunct_tokenize(sent)),\n",
    "    print(\"tweet: \", tweet_tokenize.tokenize(sent))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ply\n",
    "\n",
    "Приведем лексического анализатора на `ply`. В данном случае анализатор описывается в классе, могут быть три вида токенов - слова, цифры и пробелы. Для каждого токена в тексте выозвращается необходимая информация - типа, длина смещение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken(NUMBER,123,1,0)\n",
      "LexToken(ID,'abs',1,4)\n",
      "LexToken(NUMBER,965,1,8)\n"
     ]
    }
   ],
   "source": [
    "from ply.lex import lex, TOKEN\n",
    "\n",
    "class Lexer:\n",
    "    tokens = ( 'NUMBER', 'ID', 'WHITESPACE' )\n",
    "    \n",
    "    @TOKEN(r'\\d{1,5}')\n",
    "    def t_NUMBER(self, t):\n",
    "        t.value = int(t.value)\n",
    "        return t\n",
    "\n",
    "    @TOKEN(r'\\w+')\n",
    "    def t_ID(self, t):\n",
    "        return t\n",
    "\n",
    "    @TOKEN(r'\\s+')\n",
    "    def t_WHITESPACE(self, t):\n",
    "        pass\n",
    "\n",
    "    def t_error(self, t):\n",
    "        pass\n",
    "    \n",
    "\n",
    "__file__ = ''     # make `ply` happy\n",
    "\n",
    "lexer = lex(object=Lexer())\n",
    "lexer.input('123 abs 965')\n",
    "for token in lexer:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyparsing\n",
    "\n",
    "Другой пример pyparsing, с помощью которого можно обрабатывать более широкий класс формальных языков. С помощью специального DSL (domain-specific language, предметно-ориентированный язык) описывается грамматика. С помощью pyparsing можно обрабатывать коллекции в специфичных форматах, извлекать логи и так далее.\n",
    "\n",
    "Опишем грамматику для разбора простейших математических выражений. Сначала классы, которые описывают узлы деревьев разбора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyparsing import Word, Literal, Or, nums, Forward, StringEnd\n",
    "from operator import mul, truediv, add, sub\n",
    "\n",
    "class NumNode(object):\n",
    "    def __init__(self, t):\n",
    "        self.num = float(t[0])        \n",
    "    def calc(self):\n",
    "        return self.num          \n",
    "    def __repr__(self):\n",
    "        return 'Num(%s)' % self.num\n",
    "        \n",
    "class OpNode(object):\n",
    "    def __init__(self, t):               \n",
    "        self.left = t[0]\n",
    "        self.op = { '-' : sub, '+' : add, '/' : truediv, '*' : mul }[t[1]]\n",
    "        self.right = t[2]       \n",
    "    def calc(self):\n",
    "        return self.op(self.left.calc(), self.right.calc())        \n",
    "    def __repr__(self):\n",
    "        return 'Op(%s, %s, %s)' % (self.left, self.op, self.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "затем опишем грамматику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus = Literal('+')\n",
    "minus = Literal('-')\n",
    "div = Literal('/')\n",
    "mult = Literal('*')\n",
    "        \n",
    "factor = Word(nums).setParseAction(NumNode)\n",
    "\n",
    "term = Forward()\n",
    "term << (( factor + (mult | div) + term ).setParseAction(OpNode) | factor )        \n",
    "\n",
    "expr = Forward()\n",
    "expr << ((term + (plus | minus) + expr).setParseAction(OpNode) | term )\n",
    "\n",
    "start = expr + StringEnd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Op(Op(Num(2.0), <built-in function mul>, Num(4.0)), <built-in function add>, Op(Num(6.0), <built-in function mul>, Num(7.0)))\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "tree = start.parseString('2 * 4 + 6 * 7')[0]\n",
    "print(tree)\n",
    "print(tree.calc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
