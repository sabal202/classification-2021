{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Градиентные бустинги и сравнение популярных реализаций\r\n",
    "\r\n",
    "Подготовил: Сабалевский Сергей"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "План:\r\n",
    "\r\n",
    "1. Вспомним теорию бустингов на примере Adaboost\r\n",
    "2. Перейдем рассмотрению градиентного бустинга, какие улучшения бывают\r\n",
    "3. Сравним различия реализаций градиентного бустинга. \r\n",
    "4. Сравним их результаты на примере одного датасета:\r\n",
    " - LightGBM\r\n",
    " - CatBoost\r\n",
    " - XGBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "from sklearn import datasets as skdata # генерация примеров\r\n",
    "from sklearn import tree as sktree     # реализация алгоритмов с деревьями решений\r\n",
    "from sklearn import base as skbase     # базовые классы с реализацией API\r\n",
    "from sklearn import utils as skutils   # полезные вспомогательные функции для модели\r\n",
    "from sklearn import metrics as skmetrics\r\n",
    "\r\n",
    "from pprint import pprint\r\n",
    "\r\n",
    "from IPython.display import Image, display_svg, SVG\r\n",
    "from dtreeviz.trees import dtreeviz\r\n",
    "\r\n",
    "import scipy.optimize as sciopt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "import plotly.graph_objects as go      # визуализация\r\n",
    "import plotly.subplots as subp         # для подграфиков"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Бустинги\r\n",
    "\r\n",
    "Это процедура последовательного построения композиции алгоритмов, когда каждый следующий алгоритм стремится компенсировать недостатки композиции всех предыдущих. Если коротко, то это жадный алгоритм построения композиции алгоритмов машинного обучения.\r\n",
    "\r\n",
    "Математическая запись:\\\r\n",
    "$a(x) = sign(\\sum_{t=1}^T \\alpha_t b_t(x))$ - заданный алгоритм\\\r\n",
    "$Q_T(a,X) = \\sum_{i=1}^n [y_i\\sum_{t=i}^T \\alpha_t b_t(x_i)<0]$ - функционал ошибки\r\n",
    "\r\n",
    "для упрощения задачи минимизации функционала можно применить две эвристики:\r\n",
    "-  При добавлении в композицию слагаемого $\\alpha_tb_t(x)$ оптимизируется только базовый алгоритм $b_t$ и коэффициент при нём $α_t$, а все предыдущие слагаемые $α_1b_1(x), . . . , α_{t−1}b_{t−1}(x)$ полагаются фиксированными. \r\n",
    "- Пороговая функция потерь $Q_T$ апроксимируется сверху непрерывно дифференцируемой. \\\r\n",
    "$Q_T(a,X) = \\sum_{i=1}^n exp(-y_i\\sum_{t=i}^{T-1} \\alpha_t b_t(x_i)) * e^{-y_i\\alpha_Tb_T(x_i)}$\r\n",
    "\r\n",
    "Далее рассмотри конкретный алгоритм __Ada boost classifier__  на примере бинарной классификации:"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Достоинства алгоритма:\n",
    "- Хорошая обобщающая способность. В реальных задачах (не всегда, но часто)\n",
    "удаётся строить композиции, превосходящие по качеству базовые алгоритмы.\n",
    "Обобщающая способность может улучшаться (в некоторых задачах) по мере\n",
    "увеличения числа базовых алгоритмов.\n",
    "- Простота реализации\n",
    "- Накладные расходы бустинга невелики. Время построения композиции практически полностью определяется временем обучения базовых алгоритмов.\n",
    "- Возможность идентифицировать выбросы. \n"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Недостатки:\n",
    "- AdaBoost склонен к переобучению при наличии значительного уровня шума\n",
    "в данных. Экспоненциальная функция потерь слишком сильно увеличивает веса «наиболее трудных» объектов, на которых ошибаются многие базовые алгоритмы.\n",
    "- AdaBoost требует достаточно длинных обучающих выборок.\n",
    "- Жадная стратегия последовательного добавления приводит к построению\n",
    "неоптимального набора базовых алгоритмов.\n",
    "- Бустинг может приводить к построению громоздких композиций, состоящих\n",
    "из сотен алгоритмов. Такие композиции исключают возможность содержательной интерпретации, требуют больших объёмов памяти для хранения базовых\n",
    "алгоритмов и существенных затрат времени на вычисление классификаций."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как можно улушить алгоритм?\n",
    "- фильтрация выбросов\n",
    "- валидационная выборка\n",
    "- при накоплении ошибки возвращаться назад и переобучать базовые алгоритмы в композции\n",
    "- после обучения композиции можно провести оптимизацию весов голосования с помощью стандартных методов построения разделяющей поверхности."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Градиентные бустинги"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Первой реализацией бустинга, которая имела большой успех в применении, был Адаптивный бустинг, или сокращенно AdaBoost.\n",
    "\n",
    "Потом пришло время обобщений.\n",
    "\n",
    "Статистическая структура представляет собой задачу численной оптимизации, цель которой состоит в том, чтобы минимизировать потери модели путем добавления слабых учащихся с использованием процедуры градиентного спуска.\n",
    "\n",
    "Обобщение позволило использовать произвольные дифференцируемые функции потерь, расширив метод за пределы задач бинарной классификации для поддержки регрессии, многоклассовой классификации и многого другого.\n"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Как работает градиентный бустинг\n",
    "\n",
    "Градиентный бустинг включает в себя три элемента:\n",
    "\n",
    "1. Функция потерь, подлежащая оптимизации.\n",
    "2. Слабый ученик, чтобы делать прогнозы.\n",
    "3. Аддитивная модель для добавления слабых учащихся, чтобы минимизировать функцию потерь."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Loss Function\n",
    "\n",
    "Используемая функция потерь зависит от типа решаемой проблемы.\n",
    "\n",
    "Он должен быть дифференцируемым, но поддерживаются многие стандартные функции потерь, и вы можете определить свои собственные.\n",
    "\n",
    "Например, регрессия может использовать квадратическую ошибку, а классификация может использовать логарифмические потери.\n",
    "\n",
    "Преимущество фреймворка градиентного бустинга заключается в том, что для каждой функции потерь, которую может потребоваться использовать, не нужно выводить новый алгоритм бустинга, вместо этого это достаточно общая структура, в которой можно использовать любую дифференцируемую функцию потерь."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Weak Learner\n",
    "\n",
    "Деревья решений используются в качестве слабого ученика в градиентном бустинге.\n",
    "\n",
    "В частности, используются деревья регрессии, которые выводят реальные значения для разбиений и выходные данные которых могут быть сложены вместе, позволяя добавлять выходные данные последующих моделей и “корректировать” остатки в прогнозах.\n",
    "\n",
    "Деревья строятся жадным образом, выбирая лучшие точки разделения на основе показателей чистоты, таких как Джини, или для минимизации потерь.\n",
    "\n",
    "Первоначально, например, в случае AdaBoost, использовались очень короткие деревья решений, которые имели только одно разделение, называемое пнем решения. Большие деревья можно использовать, как правило, с уровнями от 4 до 8.\n",
    "\n",
    "Обычно слабые учащиеся ограничиваются определенными способами, такими как максимальное количество слоев, узлов, расщеплений или конечных узлов.\n",
    "\n",
    "Это делается для того, чтобы учащиеся оставались слабыми, но все еще могли быть построены жадным образом."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Additive Model\n",
    "\n",
    "Деревья добавляются по одному, и существующие деревья в модели не изменяются.\n",
    "\n",
    "Процедура градиентного спуска используется для минимизации потерь при добавлении деревьев.\n",
    "\n",
    "Традиционно градиентный спуск используется для минимизации набора параметров, таких как коэффициенты в уравнении регрессии или веса в нейронной сети. После вычисления ошибки или потери веса обновляются, чтобы минимизировать эту ошибку.\n",
    "\n",
    "Вместо параметров у нас есть слабые подмодели учащихся или, более конкретно, деревья решений. После вычисления потерь, чтобы выполнить процедуру градиентного спуска, мы должны добавить в модель дерево, которое уменьшает потери (т.е. Следовать градиенту). Мы делаем это, параметризуя дерево, затем изменяем параметры дерева и движемся в правильном направлении (уменьшая остаточные потери.\n",
    "\n",
    "Обычно этот подход называется функциональным градиентным спуском или градиентным спуском с функциями.\n",
    "\n",
    "Выходные данные для нового дерева затем добавляются к выходным данным существующей последовательности деревьев в попытке исправить или улучшить конечный результат модели.\n",
    "\n",
    "Фиксированное количество деревьев добавляется или обучение прекращается, как только потери достигают приемлемого уровня или больше не улучшаются во внешнем наборе данных проверки."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Улучшения в базовом градиентном бустинге\n",
    "\n",
    "\n",
    "Градиентный бустинг - это жадный алгоритм, который может быстро подогнать обучающий набор данных.\n",
    "\n",
    "Он может извлечь выгоду из методов регуляризации, которые штрафуют различные части алгоритма и в целом повышают производительность алгоритма за счет уменьшения переобучения.\n",
    "\n",
    "В этом разделе мы рассмотрим 4 усовершенствования базового градиентного бустинга:\n",
    "\n",
    "- Tree Constraints\n",
    "- Shrinkage (Weighted Updates, аналогия learning rate)\n",
    "- Random sampling (Stochastic Gradient Boosting)\n",
    "- Penalized Learning (L1, L2 regularization)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tree Constraints\n",
    "\n",
    "Хорошая общая эвристика заключается в том, что чем более ограничено создание дерева, тем больше деревьев вам понадобится в модели, и наоборот, чем меньше отдельных деревьев, тем меньше деревьев потребуется.\n",
    "\n",
    "Некоторые ограничения на деревья:\n",
    "\n",
    "- **Количество деревьев**, как правило, добавление большего количества деревьев в модель может быть очень медленным для перенастройки. Совет состоит в том, чтобы продолжать добавлять деревья до тех пор, пока не будет наблюдаться никакого дальнейшего улучшения.\n",
    "- **Глубина дерева**, более глубокие деревья являются более сложными деревьями, и более короткие деревья предпочтительнее. Как правило, лучшие результаты наблюдаются с 4-8 уровнями.\n",
    "- **Количество узлов или количество листьев**, например глубина, это может ограничить размер дерева, но не ограничивается симметричной структурой, если используются другие ограничения.\n",
    "- **Количество наблюдений за разбиение** накладывает минимальное ограничение на объем обучающих данных в обучающем узле, прежде чем можно будет рассмотреть разбиение"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Перейдем к примерам"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src='https://miro.medium.com/max/3076/1*i0CA9ho0WArOj-0UdpuKGQ.png' alignment='center'/>"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Структурные различия в LightGBM, XGBoost и Catboost\n",
    "\n",
    "LightGBM использует новую технику односторонней выборки на основе градиента (Gradient-based One-Side Sampling GOSS) для фильтрации экземпляров данных для нахождения значения разделения, в то время как XGBoost использует предварительно отсортированный алгоритм и алгоритм на основе гистограммы для вычисления наилучшего разделения."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### pre-sorting splitting\n",
    "\n",
    "- Для каждого узла проходимся по всем признакам\n",
    "- Для каждого признака отсортирем экземпляры по значению признака\n",
    "- Используйте линейное сканирование, чтобы решить, как лучше разделить информацию по этому признаку.\n",
    "- Возьмем лучшее решение для разделения по всем функциям\n",
    "\n",
    "Таким образом, XGBoost не использует никаких методов взвешенной выборки, что делает его процесс разделения медленнее по сравнению с GOSS и MVS."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GOSS\n",
    "\n",
    "Основное допущение, принятое здесь, состоит в том, что выборки с обучающими экземплярами с небольшими градиентами имеют меньшую ошибку обучения и уже хорошо обучены.\n",
    "\n",
    "Чтобы сохранить одинаковое распределение данных, при вычислении прироста информации GOSS вводит постоянный множитель для экземпляров данных с небольшими градиентами. Таким образом, GOSS обеспечивает хороший баланс между сокращением числа экземпляров данных и сохранением точности для изученных деревьев решений."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Catboost и его MVS\n",
    "\n",
    "Catboost предлагает новый метод, называемый выборкой с минимальной дисперсией (MVS), который представляет собой взвешенную версию стохастического градиентного бустинга. В этом методе взвешенная выборка происходит на уровне дерева, а не на уровне разделения. Наблюдения для каждого дерева бустинга отбираются таким образом, чтобы максимизировать точность разделения оценок."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Catboost и небрежные (oblivious) деревья решений\n",
    "\n",
    "Чтобы вырастить сбалансированное дерево. Одни и те же функции используются для создания левых и правых разделений (split) на каждом уровне дерева.\n",
    "\n",
    "По сравнению с классическими деревьями, небрежные деревья более эффективны при реализации на процессоре и просты в обучении."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Сходство в гиперпараметрах\r\n",
    "\r\n",
    "<img src='https://miro.medium.com/max/3400/1*A0b_ahXOrrijazzJengwYw.png' width=1000 height=700 alignment='center'/>"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Реализация методов на датасете\r\n",
    "\r\n",
    "Воспользуемся набором данных Kaggle о задержках рейсов за 2015 год, поскольку он имеет как категориальные, \r\n",
    "так и числовые характеристики. Имея примерно 5 миллионов строк, этот набор данных будет полезен для \r\n",
    "оценки производительности с точки зрения скорости и точности настроенных моделей для каждого типа повышения. \r\n",
    "Будем использовать 10% подмножества этих данных ~ 500 тыс. строк.\r\n",
    "\r\n",
    "Ниже приведены признаки, используемые для моделирования:\r\n",
    "    \r\n",
    "- MONTH, DAY, DAY_OF_WEEK: data type int\r\n",
    "- AIRLINE and FLIGHT_NUMBER: data type int\r\n",
    "- ORIGIN_AIRPORT and DESTINATION_AIRPORT: data type string\r\n",
    "- DEPARTURE_TIME: data type float\r\n",
    "- ARRIVAL_DELAY: this will be the target and is transformed into boolean variable indicating delay of more than 10 minutes\r\n",
    "- DISTANCE and AIR_TIME: data type float"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import xgboost as xgb\r\n",
    "from sklearn import metrics\r\n",
    "import lightgbm as lgb\r\n",
    "import catboost as cb"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "dataset = pd.read_csv(\"../data/flights.csv\")\r\n",
    "dataset.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Anaconda3\\envs\\classification2021\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning:\n",
      "\n",
      "Columns (7,8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5819079 entries, 0 to 5819078\n",
      "Data columns (total 31 columns):\n",
      " #   Column               Dtype  \n",
      "---  ------               -----  \n",
      " 0   YEAR                 int64  \n",
      " 1   MONTH                int64  \n",
      " 2   DAY                  int64  \n",
      " 3   DAY_OF_WEEK          int64  \n",
      " 4   AIRLINE              object \n",
      " 5   FLIGHT_NUMBER        int64  \n",
      " 6   TAIL_NUMBER          object \n",
      " 7   ORIGIN_AIRPORT       object \n",
      " 8   DESTINATION_AIRPORT  object \n",
      " 9   SCHEDULED_DEPARTURE  int64  \n",
      " 10  DEPARTURE_TIME       float64\n",
      " 11  DEPARTURE_DELAY      float64\n",
      " 12  TAXI_OUT             float64\n",
      " 13  WHEELS_OFF           float64\n",
      " 14  SCHEDULED_TIME       float64\n",
      " 15  ELAPSED_TIME         float64\n",
      " 16  AIR_TIME             float64\n",
      " 17  DISTANCE             int64  \n",
      " 18  WHEELS_ON            float64\n",
      " 19  TAXI_IN              float64\n",
      " 20  SCHEDULED_ARRIVAL    int64  \n",
      " 21  ARRIVAL_TIME         float64\n",
      " 22  ARRIVAL_DELAY        float64\n",
      " 23  DIVERTED             int64  \n",
      " 24  CANCELLED            int64  \n",
      " 25  CANCELLATION_REASON  object \n",
      " 26  AIR_SYSTEM_DELAY     float64\n",
      " 27  SECURITY_DELAY       float64\n",
      " 28  AIRLINE_DELAY        float64\n",
      " 29  LATE_AIRCRAFT_DELAY  float64\n",
      " 30  WEATHER_DELAY        float64\n",
      "dtypes: float64(16), int64(10), object(5)\n",
      "memory usage: 1.3+ GB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "data = dataset.sample(frac=0.1, random_state=10)\r\n",
    "\r\n",
    "data = data[[\"MONTH\", \"DAY\", \"DAY_OF_WEEK\", \"AIRLINE\", \"FLIGHT_NUMBER\", \"DESTINATION_AIRPORT\",\r\n",
    "             \"ORIGIN_AIRPORT\", \"AIR_TIME\", \"DEPARTURE_TIME\", \"DISTANCE\", \"ARRIVAL_DELAY\"]]\r\n",
    "data.dropna(inplace=True)\r\n",
    "\r\n",
    "data[\"ARRIVAL_DELAY\"] = (data[\"ARRIVAL_DELAY\"] > 10)*1\r\n",
    "\r\n",
    "cols = [\"AIRLINE\", \"FLIGHT_NUMBER\", \"DESTINATION_AIRPORT\", \"ORIGIN_AIRPORT\"]\r\n",
    "for item in cols:\r\n",
    "    data[item] = data[item].astype(\"category\").cat.codes + 1\r\n",
    "\r\n",
    "train, test, y_train, y_test = train_test_split(data.drop([\"ARRIVAL_DELAY\"], axis=1), data[\"ARRIVAL_DELAY\"],\r\n",
    "                                                random_state=10, test_size=0.25)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "len(train), len(test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(428504, 142835)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def auc_from_pred(m, train, test):\r\n",
    "    return (\r\n",
    "        metrics.roc_auc_score(y_train, m.predict(train)),\r\n",
    "        metrics.roc_auc_score(y_test, m.predict(test)),\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def auc_from_proba(m, train, test):\r\n",
    "    return (\r\n",
    "        metrics.roc_auc_score(y_train, m.predict_proba(train)[:, 1]),\r\n",
    "        metrics.roc_auc_score(y_test, m.predict_proba(test)[:, 1]),\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "model = xgb.XGBClassifier(\r\n",
    "    max_depth=50,\r\n",
    "    min_child_weight=1,\r\n",
    "    n_estimators=200,\r\n",
    "    n_jobs=-1,\r\n",
    "    verbose=1,\r\n",
    "    learning_rate=0.16,\r\n",
    ")\r\n",
    "model.fit(train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Anaconda3\\envs\\classification2021\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[16:35:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:35:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.16, max_delta_step=0, max_depth=50,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=200, n_jobs=-1, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbose=1,\n",
       "              verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "auc_xgb = auc_from_proba(model, train, test)\r\n",
    "print(np.round(auc_xgb, 3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.    0.789]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Light GBM"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "d_train = lgb.Dataset(train, label=y_train, free_raw_data=False)\r\n",
    "params = {\"max_depth\": 50, \"learning_rate\": 0.1, \"num_leaves\": 900, \"n_estimators\": 300}"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# Without Categorical Features\r\n",
    "model1_lgmb = lgb.train(params, d_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Anaconda3\\envs\\classification2021\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning:\n",
      "\n",
      "Found `n_estimators` in params. Will use it instead of argument\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1524\n",
      "[LightGBM] [Info] Number of data points in the train set: 428504, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.219146\n"
     ]
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "auc_lgbm1 = auc_from_pred(model1_lgmb, train, test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# With Catgeorical Features\r\n",
    "cate_features_name = [\r\n",
    "    \"MONTH\",\r\n",
    "    \"DAY\",\r\n",
    "    \"DAY_OF_WEEK\",\r\n",
    "    \"AIRLINE\",\r\n",
    "    \"DESTINATION_AIRPORT\",\r\n",
    "    \"ORIGIN_AIRPORT\",\r\n",
    "]\r\n",
    "model2_lgmb = lgb.train(params, d_train, categorical_feature=cate_features_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Anaconda3\\envs\\classification2021\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning:\n",
      "\n",
      "Found `n_estimators` in params. Will use it instead of argument\n",
      "\n",
      "C:\\Anaconda3\\envs\\classification2021\\lib\\site-packages\\lightgbm\\basic.py:1705: UserWarning:\n",
      "\n",
      "categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['AIRLINE', 'DAY', 'DAY_OF_WEEK', 'DESTINATION_AIRPORT', 'MONTH', 'ORIGIN_AIRPORT']\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1897\n",
      "[LightGBM] [Info] Number of data points in the train set: 428504, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.219146\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "auc_lgbm2 = auc_from_pred(model2_lgmb, train, test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "print(np.round(auc_lgbm1, 3))\r\n",
    "print(np.round(auc_lgbm2, 3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.981 0.785]\n",
      "[0.998 0.775]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CatBoost\n",
    "\n",
    "При настройке параметров для CatBoost трудно передать индексы для категориальных функций. \n",
    "Поэтому настроим параметры без передачи категориальных признаков и оценим две модели — одну с категориальными признаками, а другую без них. \n"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# Without Categorical features\r\n",
    "clf1 = cb.CatBoostClassifier(\r\n",
    "    eval_metric=\"AUC\", depth=10, iterations=500, l2_leaf_reg=9, learning_rate=0.15\r\n",
    ")\r\n",
    "clf1.fit(train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:\ttotal: 309ms\tremaining: 2m 33s\n",
      "1:\ttotal: 486ms\tremaining: 2m\n",
      "2:\ttotal: 664ms\tremaining: 1m 50s\n",
      "3:\ttotal: 861ms\tremaining: 1m 46s\n",
      "4:\ttotal: 1.05s\tremaining: 1m 44s\n",
      "5:\ttotal: 1.26s\tremaining: 1m 43s\n",
      "6:\ttotal: 1.46s\tremaining: 1m 42s\n",
      "7:\ttotal: 1.67s\tremaining: 1m 42s\n",
      "8:\ttotal: 1.9s\tremaining: 1m 43s\n",
      "9:\ttotal: 2.11s\tremaining: 1m 43s\n",
      "10:\ttotal: 2.28s\tremaining: 1m 41s\n",
      "11:\ttotal: 2.47s\tremaining: 1m 40s\n",
      "12:\ttotal: 2.64s\tremaining: 1m 38s\n",
      "13:\ttotal: 2.81s\tremaining: 1m 37s\n",
      "14:\ttotal: 2.97s\tremaining: 1m 36s\n",
      "15:\ttotal: 3.12s\tremaining: 1m 34s\n",
      "16:\ttotal: 3.28s\tremaining: 1m 33s\n",
      "17:\ttotal: 3.43s\tremaining: 1m 31s\n",
      "18:\ttotal: 3.59s\tremaining: 1m 30s\n",
      "19:\ttotal: 3.76s\tremaining: 1m 30s\n",
      "20:\ttotal: 3.94s\tremaining: 1m 29s\n",
      "21:\ttotal: 4.1s\tremaining: 1m 29s\n",
      "22:\ttotal: 4.25s\tremaining: 1m 28s\n",
      "23:\ttotal: 4.4s\tremaining: 1m 27s\n",
      "24:\ttotal: 4.56s\tremaining: 1m 26s\n",
      "25:\ttotal: 4.72s\tremaining: 1m 26s\n",
      "26:\ttotal: 4.89s\tremaining: 1m 25s\n",
      "27:\ttotal: 5.07s\tremaining: 1m 25s\n",
      "28:\ttotal: 5.23s\tremaining: 1m 24s\n",
      "29:\ttotal: 5.38s\tremaining: 1m 24s\n",
      "30:\ttotal: 5.52s\tremaining: 1m 23s\n",
      "31:\ttotal: 5.66s\tremaining: 1m 22s\n",
      "32:\ttotal: 5.8s\tremaining: 1m 22s\n",
      "33:\ttotal: 5.97s\tremaining: 1m 21s\n",
      "34:\ttotal: 6.14s\tremaining: 1m 21s\n",
      "35:\ttotal: 6.31s\tremaining: 1m 21s\n",
      "36:\ttotal: 6.45s\tremaining: 1m 20s\n",
      "37:\ttotal: 6.58s\tremaining: 1m 20s\n",
      "38:\ttotal: 6.74s\tremaining: 1m 19s\n",
      "39:\ttotal: 6.9s\tremaining: 1m 19s\n",
      "40:\ttotal: 7.04s\tremaining: 1m 18s\n",
      "41:\ttotal: 7.2s\tremaining: 1m 18s\n",
      "42:\ttotal: 7.39s\tremaining: 1m 18s\n",
      "43:\ttotal: 7.56s\tremaining: 1m 18s\n",
      "44:\ttotal: 7.71s\tremaining: 1m 17s\n",
      "45:\ttotal: 7.85s\tremaining: 1m 17s\n",
      "46:\ttotal: 8.01s\tremaining: 1m 17s\n",
      "47:\ttotal: 8.16s\tremaining: 1m 16s\n",
      "48:\ttotal: 8.31s\tremaining: 1m 16s\n",
      "49:\ttotal: 8.48s\tremaining: 1m 16s\n",
      "50:\ttotal: 8.64s\tremaining: 1m 16s\n",
      "51:\ttotal: 8.81s\tremaining: 1m 15s\n",
      "52:\ttotal: 9s\tremaining: 1m 15s\n",
      "53:\ttotal: 9.18s\tremaining: 1m 15s\n",
      "54:\ttotal: 9.35s\tremaining: 1m 15s\n",
      "55:\ttotal: 9.5s\tremaining: 1m 15s\n",
      "56:\ttotal: 9.68s\tremaining: 1m 15s\n",
      "57:\ttotal: 9.9s\tremaining: 1m 15s\n",
      "58:\ttotal: 10.1s\tremaining: 1m 15s\n",
      "59:\ttotal: 10.4s\tremaining: 1m 16s\n",
      "60:\ttotal: 10.6s\tremaining: 1m 16s\n",
      "61:\ttotal: 10.9s\tremaining: 1m 16s\n",
      "62:\ttotal: 11.2s\tremaining: 1m 17s\n",
      "63:\ttotal: 11.4s\tremaining: 1m 17s\n",
      "64:\ttotal: 11.6s\tremaining: 1m 17s\n",
      "65:\ttotal: 11.9s\tremaining: 1m 17s\n",
      "66:\ttotal: 12.2s\tremaining: 1m 18s\n",
      "67:\ttotal: 12.5s\tremaining: 1m 19s\n",
      "68:\ttotal: 12.7s\tremaining: 1m 19s\n",
      "69:\ttotal: 13s\tremaining: 1m 19s\n",
      "70:\ttotal: 13.2s\tremaining: 1m 19s\n",
      "71:\ttotal: 13.5s\tremaining: 1m 20s\n",
      "72:\ttotal: 13.8s\tremaining: 1m 20s\n",
      "73:\ttotal: 14.1s\tremaining: 1m 21s\n",
      "74:\ttotal: 14.3s\tremaining: 1m 21s\n",
      "75:\ttotal: 14.6s\tremaining: 1m 21s\n",
      "76:\ttotal: 14.8s\tremaining: 1m 21s\n",
      "77:\ttotal: 15.1s\tremaining: 1m 21s\n",
      "78:\ttotal: 15.3s\tremaining: 1m 21s\n",
      "79:\ttotal: 15.5s\tremaining: 1m 21s\n",
      "80:\ttotal: 15.8s\tremaining: 1m 21s\n",
      "81:\ttotal: 16s\tremaining: 1m 21s\n",
      "82:\ttotal: 16.3s\tremaining: 1m 21s\n",
      "83:\ttotal: 16.5s\tremaining: 1m 21s\n",
      "84:\ttotal: 16.7s\tremaining: 1m 21s\n",
      "85:\ttotal: 16.9s\tremaining: 1m 21s\n",
      "86:\ttotal: 17.1s\tremaining: 1m 21s\n",
      "87:\ttotal: 17.3s\tremaining: 1m 20s\n",
      "88:\ttotal: 17.5s\tremaining: 1m 20s\n",
      "89:\ttotal: 17.6s\tremaining: 1m 20s\n",
      "90:\ttotal: 17.8s\tremaining: 1m 20s\n",
      "91:\ttotal: 18s\tremaining: 1m 19s\n",
      "92:\ttotal: 18.2s\tremaining: 1m 19s\n",
      "93:\ttotal: 18.3s\tremaining: 1m 19s\n",
      "94:\ttotal: 18.5s\tremaining: 1m 18s\n",
      "95:\ttotal: 18.6s\tremaining: 1m 18s\n",
      "96:\ttotal: 18.8s\tremaining: 1m 18s\n",
      "97:\ttotal: 18.9s\tremaining: 1m 17s\n",
      "98:\ttotal: 19.1s\tremaining: 1m 17s\n",
      "99:\ttotal: 19.2s\tremaining: 1m 16s\n",
      "100:\ttotal: 19.4s\tremaining: 1m 16s\n",
      "101:\ttotal: 19.6s\tremaining: 1m 16s\n",
      "102:\ttotal: 19.7s\tremaining: 1m 16s\n",
      "103:\ttotal: 19.9s\tremaining: 1m 15s\n",
      "104:\ttotal: 20s\tremaining: 1m 15s\n",
      "105:\ttotal: 20.2s\tremaining: 1m 15s\n",
      "106:\ttotal: 20.4s\tremaining: 1m 14s\n",
      "107:\ttotal: 20.5s\tremaining: 1m 14s\n",
      "108:\ttotal: 20.7s\tremaining: 1m 14s\n",
      "109:\ttotal: 20.8s\tremaining: 1m 13s\n",
      "110:\ttotal: 21s\tremaining: 1m 13s\n",
      "111:\ttotal: 21.1s\tremaining: 1m 13s\n",
      "112:\ttotal: 21.3s\tremaining: 1m 12s\n",
      "113:\ttotal: 21.5s\tremaining: 1m 12s\n",
      "114:\ttotal: 21.6s\tremaining: 1m 12s\n",
      "115:\ttotal: 21.8s\tremaining: 1m 12s\n",
      "116:\ttotal: 21.9s\tremaining: 1m 11s\n",
      "117:\ttotal: 22.1s\tremaining: 1m 11s\n",
      "118:\ttotal: 22.2s\tremaining: 1m 11s\n",
      "119:\ttotal: 22.4s\tremaining: 1m 10s\n",
      "120:\ttotal: 22.5s\tremaining: 1m 10s\n",
      "121:\ttotal: 22.7s\tremaining: 1m 10s\n",
      "122:\ttotal: 22.8s\tremaining: 1m 9s\n",
      "123:\ttotal: 23s\tremaining: 1m 9s\n",
      "124:\ttotal: 23.1s\tremaining: 1m 9s\n",
      "125:\ttotal: 23.3s\tremaining: 1m 9s\n",
      "126:\ttotal: 23.4s\tremaining: 1m 8s\n",
      "127:\ttotal: 23.6s\tremaining: 1m 8s\n",
      "128:\ttotal: 23.7s\tremaining: 1m 8s\n",
      "129:\ttotal: 23.9s\tremaining: 1m 8s\n",
      "130:\ttotal: 24.1s\tremaining: 1m 7s\n",
      "131:\ttotal: 24.3s\tremaining: 1m 7s\n",
      "132:\ttotal: 24.4s\tremaining: 1m 7s\n",
      "133:\ttotal: 24.6s\tremaining: 1m 7s\n",
      "134:\ttotal: 24.8s\tremaining: 1m 6s\n",
      "135:\ttotal: 24.9s\tremaining: 1m 6s\n",
      "136:\ttotal: 25.1s\tremaining: 1m 6s\n",
      "137:\ttotal: 25.3s\tremaining: 1m 6s\n",
      "138:\ttotal: 25.4s\tremaining: 1m 6s\n",
      "139:\ttotal: 25.6s\tremaining: 1m 5s\n",
      "140:\ttotal: 25.8s\tremaining: 1m 5s\n",
      "141:\ttotal: 26s\tremaining: 1m 5s\n",
      "142:\ttotal: 26.2s\tremaining: 1m 5s\n",
      "143:\ttotal: 26.4s\tremaining: 1m 5s\n",
      "144:\ttotal: 26.5s\tremaining: 1m 4s\n",
      "145:\ttotal: 26.7s\tremaining: 1m 4s\n",
      "146:\ttotal: 26.9s\tremaining: 1m 4s\n",
      "147:\ttotal: 27.1s\tremaining: 1m 4s\n",
      "148:\ttotal: 27.2s\tremaining: 1m 4s\n",
      "149:\ttotal: 27.4s\tremaining: 1m 3s\n",
      "150:\ttotal: 27.6s\tremaining: 1m 3s\n",
      "151:\ttotal: 27.7s\tremaining: 1m 3s\n",
      "152:\ttotal: 27.9s\tremaining: 1m 3s\n",
      "153:\ttotal: 28.1s\tremaining: 1m 3s\n",
      "154:\ttotal: 28.3s\tremaining: 1m 2s\n",
      "155:\ttotal: 28.4s\tremaining: 1m 2s\n",
      "156:\ttotal: 28.6s\tremaining: 1m 2s\n",
      "157:\ttotal: 28.7s\tremaining: 1m 2s\n",
      "158:\ttotal: 28.9s\tremaining: 1m 1s\n",
      "159:\ttotal: 29.1s\tremaining: 1m 1s\n",
      "160:\ttotal: 29.3s\tremaining: 1m 1s\n",
      "161:\ttotal: 29.5s\tremaining: 1m 1s\n",
      "162:\ttotal: 29.7s\tremaining: 1m 1s\n",
      "163:\ttotal: 29.8s\tremaining: 1m 1s\n",
      "164:\ttotal: 30s\tremaining: 1m\n",
      "165:\ttotal: 30.2s\tremaining: 1m\n",
      "166:\ttotal: 30.4s\tremaining: 1m\n",
      "167:\ttotal: 30.6s\tremaining: 1m\n",
      "168:\ttotal: 30.8s\tremaining: 1m\n",
      "169:\ttotal: 30.9s\tremaining: 1m\n",
      "170:\ttotal: 31.1s\tremaining: 59.9s\n",
      "171:\ttotal: 31.3s\tremaining: 59.7s\n",
      "172:\ttotal: 31.6s\tremaining: 59.7s\n",
      "173:\ttotal: 31.8s\tremaining: 59.6s\n",
      "174:\ttotal: 32s\tremaining: 59.5s\n",
      "175:\ttotal: 32.3s\tremaining: 59.4s\n",
      "176:\ttotal: 32.5s\tremaining: 59.3s\n",
      "177:\ttotal: 32.7s\tremaining: 59.1s\n",
      "178:\ttotal: 33s\tremaining: 59.2s\n",
      "179:\ttotal: 33.3s\tremaining: 59.1s\n",
      "180:\ttotal: 33.5s\tremaining: 59s\n",
      "181:\ttotal: 33.8s\tremaining: 59s\n",
      "182:\ttotal: 34.1s\tremaining: 59s\n",
      "183:\ttotal: 34.4s\tremaining: 59s\n",
      "184:\ttotal: 34.5s\tremaining: 58.8s\n",
      "185:\ttotal: 34.8s\tremaining: 58.7s\n",
      "186:\ttotal: 35s\tremaining: 58.6s\n",
      "187:\ttotal: 35.2s\tremaining: 58.5s\n",
      "188:\ttotal: 35.4s\tremaining: 58.3s\n",
      "189:\ttotal: 35.6s\tremaining: 58.1s\n",
      "190:\ttotal: 35.9s\tremaining: 58s\n",
      "191:\ttotal: 36.1s\tremaining: 58s\n",
      "192:\ttotal: 36.3s\tremaining: 57.8s\n",
      "193:\ttotal: 36.6s\tremaining: 57.7s\n",
      "194:\ttotal: 36.8s\tremaining: 57.5s\n",
      "195:\ttotal: 37s\tremaining: 57.4s\n",
      "196:\ttotal: 37.2s\tremaining: 57.2s\n",
      "197:\ttotal: 37.4s\tremaining: 57s\n",
      "198:\ttotal: 37.5s\tremaining: 56.7s\n",
      "199:\ttotal: 37.7s\tremaining: 56.5s\n",
      "200:\ttotal: 37.8s\tremaining: 56.3s\n",
      "201:\ttotal: 38s\tremaining: 56.1s\n",
      "202:\ttotal: 38.3s\tremaining: 56s\n",
      "203:\ttotal: 38.5s\tremaining: 55.8s\n",
      "204:\ttotal: 38.6s\tremaining: 55.6s\n",
      "205:\ttotal: 38.9s\tremaining: 55.5s\n",
      "206:\ttotal: 39.1s\tremaining: 55.4s\n",
      "207:\ttotal: 39.3s\tremaining: 55.2s\n",
      "208:\ttotal: 39.5s\tremaining: 55s\n",
      "209:\ttotal: 39.7s\tremaining: 54.8s\n",
      "210:\ttotal: 39.9s\tremaining: 54.6s\n",
      "211:\ttotal: 40.1s\tremaining: 54.5s\n",
      "212:\ttotal: 40.2s\tremaining: 54.2s\n",
      "213:\ttotal: 40.4s\tremaining: 54s\n",
      "214:\ttotal: 40.6s\tremaining: 53.8s\n",
      "215:\ttotal: 40.8s\tremaining: 53.6s\n",
      "216:\ttotal: 41s\tremaining: 53.4s\n",
      "217:\ttotal: 41.2s\tremaining: 53.2s\n",
      "218:\ttotal: 41.3s\tremaining: 53s\n",
      "219:\ttotal: 41.5s\tremaining: 52.8s\n",
      "220:\ttotal: 41.7s\tremaining: 52.6s\n",
      "221:\ttotal: 41.9s\tremaining: 52.4s\n",
      "222:\ttotal: 42.1s\tremaining: 52.3s\n",
      "223:\ttotal: 42.3s\tremaining: 52.1s\n",
      "224:\ttotal: 42.5s\tremaining: 51.9s\n",
      "225:\ttotal: 42.7s\tremaining: 51.7s\n",
      "226:\ttotal: 42.8s\tremaining: 51.5s\n",
      "227:\ttotal: 43s\tremaining: 51.3s\n",
      "228:\ttotal: 43.2s\tremaining: 51.1s\n",
      "229:\ttotal: 43.4s\tremaining: 50.9s\n",
      "230:\ttotal: 43.6s\tremaining: 50.7s\n",
      "231:\ttotal: 43.7s\tremaining: 50.5s\n",
      "232:\ttotal: 43.9s\tremaining: 50.3s\n",
      "233:\ttotal: 44.1s\tremaining: 50.1s\n",
      "234:\ttotal: 44.3s\tremaining: 50s\n",
      "235:\ttotal: 44.5s\tremaining: 49.7s\n",
      "236:\ttotal: 44.6s\tremaining: 49.5s\n",
      "237:\ttotal: 44.8s\tremaining: 49.3s\n",
      "238:\ttotal: 45s\tremaining: 49.1s\n",
      "239:\ttotal: 45.2s\tremaining: 48.9s\n",
      "240:\ttotal: 45.3s\tremaining: 48.7s\n",
      "241:\ttotal: 45.5s\tremaining: 48.5s\n",
      "242:\ttotal: 45.6s\tremaining: 48.3s\n",
      "243:\ttotal: 45.8s\tremaining: 48.1s\n",
      "244:\ttotal: 46s\tremaining: 47.8s\n",
      "245:\ttotal: 46.1s\tremaining: 47.6s\n",
      "246:\ttotal: 46.3s\tremaining: 47.4s\n",
      "247:\ttotal: 46.4s\tremaining: 47.2s\n",
      "248:\ttotal: 46.6s\tremaining: 47s\n",
      "249:\ttotal: 46.8s\tremaining: 46.8s\n",
      "250:\ttotal: 47s\tremaining: 46.6s\n",
      "251:\ttotal: 47.2s\tremaining: 46.4s\n",
      "252:\ttotal: 47.3s\tremaining: 46.2s\n",
      "253:\ttotal: 47.5s\tremaining: 46s\n",
      "254:\ttotal: 47.7s\tremaining: 45.8s\n",
      "255:\ttotal: 47.9s\tremaining: 45.6s\n",
      "256:\ttotal: 48s\tremaining: 45.4s\n",
      "257:\ttotal: 48.2s\tremaining: 45.2s\n",
      "258:\ttotal: 48.4s\tremaining: 45s\n",
      "259:\ttotal: 48.5s\tremaining: 44.8s\n",
      "260:\ttotal: 48.7s\tremaining: 44.6s\n",
      "261:\ttotal: 48.9s\tremaining: 44.4s\n",
      "262:\ttotal: 49s\tremaining: 44.2s\n",
      "263:\ttotal: 49.2s\tremaining: 44s\n",
      "264:\ttotal: 49.4s\tremaining: 43.8s\n",
      "265:\ttotal: 49.6s\tremaining: 43.6s\n",
      "266:\ttotal: 49.8s\tremaining: 43.4s\n",
      "267:\ttotal: 49.9s\tremaining: 43.2s\n",
      "268:\ttotal: 50.1s\tremaining: 43s\n",
      "269:\ttotal: 50.3s\tremaining: 42.8s\n",
      "270:\ttotal: 50.4s\tremaining: 42.6s\n",
      "271:\ttotal: 50.6s\tremaining: 42.4s\n",
      "272:\ttotal: 50.8s\tremaining: 42.2s\n",
      "273:\ttotal: 50.9s\tremaining: 42s\n",
      "274:\ttotal: 51.1s\tremaining: 41.8s\n",
      "275:\ttotal: 51.3s\tremaining: 41.6s\n",
      "276:\ttotal: 51.4s\tremaining: 41.4s\n",
      "277:\ttotal: 51.6s\tremaining: 41.2s\n",
      "278:\ttotal: 51.8s\tremaining: 41s\n",
      "279:\ttotal: 51.9s\tremaining: 40.8s\n",
      "280:\ttotal: 52.1s\tremaining: 40.6s\n",
      "281:\ttotal: 52.2s\tremaining: 40.4s\n",
      "282:\ttotal: 52.4s\tremaining: 40.2s\n",
      "283:\ttotal: 52.5s\tremaining: 40s\n",
      "284:\ttotal: 52.7s\tremaining: 39.8s\n",
      "285:\ttotal: 52.9s\tremaining: 39.5s\n",
      "286:\ttotal: 53s\tremaining: 39.3s\n",
      "287:\ttotal: 53.2s\tremaining: 39.1s\n",
      "288:\ttotal: 53.3s\tremaining: 38.9s\n",
      "289:\ttotal: 53.5s\tremaining: 38.7s\n",
      "290:\ttotal: 53.6s\tremaining: 38.5s\n",
      "291:\ttotal: 53.8s\tremaining: 38.3s\n",
      "292:\ttotal: 53.9s\tremaining: 38.1s\n",
      "293:\ttotal: 54.1s\tremaining: 37.9s\n",
      "294:\ttotal: 54.3s\tremaining: 37.7s\n",
      "295:\ttotal: 54.4s\tremaining: 37.5s\n",
      "296:\ttotal: 54.6s\tremaining: 37.3s\n",
      "297:\ttotal: 54.8s\tremaining: 37.1s\n",
      "298:\ttotal: 54.9s\tremaining: 36.9s\n",
      "299:\ttotal: 55.1s\tremaining: 36.7s\n",
      "300:\ttotal: 55.3s\tremaining: 36.5s\n",
      "301:\ttotal: 55.4s\tremaining: 36.3s\n",
      "302:\ttotal: 55.6s\tremaining: 36.1s\n",
      "303:\ttotal: 55.7s\tremaining: 35.9s\n",
      "304:\ttotal: 55.9s\tremaining: 35.7s\n",
      "305:\ttotal: 56s\tremaining: 35.5s\n",
      "306:\ttotal: 56.2s\tremaining: 35.3s\n",
      "307:\ttotal: 56.3s\tremaining: 35.1s\n",
      "308:\ttotal: 56.5s\tremaining: 34.9s\n",
      "309:\ttotal: 56.7s\tremaining: 34.7s\n",
      "310:\ttotal: 56.8s\tremaining: 34.5s\n",
      "311:\ttotal: 57s\tremaining: 34.4s\n",
      "312:\ttotal: 57.2s\tremaining: 34.2s\n",
      "313:\ttotal: 57.4s\tremaining: 34s\n",
      "314:\ttotal: 57.6s\tremaining: 33.8s\n",
      "315:\ttotal: 57.7s\tremaining: 33.6s\n",
      "316:\ttotal: 57.9s\tremaining: 33.4s\n",
      "317:\ttotal: 58.1s\tremaining: 33.3s\n",
      "318:\ttotal: 58.3s\tremaining: 33.1s\n",
      "319:\ttotal: 58.4s\tremaining: 32.9s\n",
      "320:\ttotal: 58.6s\tremaining: 32.7s\n",
      "321:\ttotal: 58.8s\tremaining: 32.5s\n",
      "322:\ttotal: 58.9s\tremaining: 32.3s\n",
      "323:\ttotal: 59.1s\tremaining: 32.1s\n",
      "324:\ttotal: 59.2s\tremaining: 31.9s\n",
      "325:\ttotal: 59.4s\tremaining: 31.7s\n",
      "326:\ttotal: 59.5s\tremaining: 31.5s\n",
      "327:\ttotal: 59.7s\tremaining: 31.3s\n",
      "328:\ttotal: 59.9s\tremaining: 31.1s\n",
      "329:\ttotal: 1m\tremaining: 31s\n",
      "330:\ttotal: 1m\tremaining: 30.8s\n",
      "331:\ttotal: 1m\tremaining: 30.6s\n",
      "332:\ttotal: 1m\tremaining: 30.4s\n",
      "333:\ttotal: 1m\tremaining: 30.2s\n",
      "334:\ttotal: 1m\tremaining: 30s\n",
      "335:\ttotal: 1m 1s\tremaining: 29.8s\n",
      "336:\ttotal: 1m 1s\tremaining: 29.6s\n",
      "337:\ttotal: 1m 1s\tremaining: 29.4s\n",
      "338:\ttotal: 1m 1s\tremaining: 29.2s\n",
      "339:\ttotal: 1m 1s\tremaining: 29s\n",
      "340:\ttotal: 1m 1s\tremaining: 28.8s\n",
      "341:\ttotal: 1m 1s\tremaining: 28.6s\n",
      "342:\ttotal: 1m 2s\tremaining: 28.4s\n",
      "343:\ttotal: 1m 2s\tremaining: 28.2s\n",
      "344:\ttotal: 1m 2s\tremaining: 28s\n",
      "345:\ttotal: 1m 2s\tremaining: 27.8s\n",
      "346:\ttotal: 1m 2s\tremaining: 27.6s\n",
      "347:\ttotal: 1m 2s\tremaining: 27.4s\n",
      "348:\ttotal: 1m 3s\tremaining: 27.3s\n",
      "349:\ttotal: 1m 3s\tremaining: 27.1s\n",
      "350:\ttotal: 1m 3s\tremaining: 26.9s\n",
      "351:\ttotal: 1m 3s\tremaining: 26.7s\n",
      "352:\ttotal: 1m 3s\tremaining: 26.5s\n",
      "353:\ttotal: 1m 3s\tremaining: 26.3s\n",
      "354:\ttotal: 1m 4s\tremaining: 26.2s\n",
      "355:\ttotal: 1m 4s\tremaining: 26s\n",
      "356:\ttotal: 1m 4s\tremaining: 25.8s\n",
      "357:\ttotal: 1m 4s\tremaining: 25.6s\n",
      "358:\ttotal: 1m 4s\tremaining: 25.4s\n",
      "359:\ttotal: 1m 4s\tremaining: 25.2s\n",
      "360:\ttotal: 1m 4s\tremaining: 25s\n",
      "361:\ttotal: 1m 5s\tremaining: 24.9s\n",
      "362:\ttotal: 1m 5s\tremaining: 24.7s\n",
      "363:\ttotal: 1m 5s\tremaining: 24.5s\n",
      "364:\ttotal: 1m 5s\tremaining: 24.3s\n",
      "365:\ttotal: 1m 5s\tremaining: 24.1s\n",
      "366:\ttotal: 1m 5s\tremaining: 23.9s\n",
      "367:\ttotal: 1m 6s\tremaining: 23.7s\n",
      "368:\ttotal: 1m 6s\tremaining: 23.6s\n",
      "369:\ttotal: 1m 6s\tremaining: 23.4s\n",
      "370:\ttotal: 1m 6s\tremaining: 23.2s\n",
      "371:\ttotal: 1m 6s\tremaining: 23s\n",
      "372:\ttotal: 1m 7s\tremaining: 22.8s\n",
      "373:\ttotal: 1m 7s\tremaining: 22.7s\n",
      "374:\ttotal: 1m 7s\tremaining: 22.5s\n",
      "375:\ttotal: 1m 7s\tremaining: 22.3s\n",
      "376:\ttotal: 1m 7s\tremaining: 22.1s\n",
      "377:\ttotal: 1m 7s\tremaining: 21.9s\n",
      "378:\ttotal: 1m 8s\tremaining: 21.7s\n",
      "379:\ttotal: 1m 8s\tremaining: 21.5s\n",
      "380:\ttotal: 1m 8s\tremaining: 21.4s\n",
      "381:\ttotal: 1m 8s\tremaining: 21.2s\n",
      "382:\ttotal: 1m 8s\tremaining: 21s\n",
      "383:\ttotal: 1m 8s\tremaining: 20.8s\n",
      "384:\ttotal: 1m 9s\tremaining: 20.7s\n",
      "385:\ttotal: 1m 9s\tremaining: 20.5s\n",
      "386:\ttotal: 1m 9s\tremaining: 20.3s\n",
      "387:\ttotal: 1m 9s\tremaining: 20.1s\n",
      "388:\ttotal: 1m 9s\tremaining: 19.9s\n",
      "389:\ttotal: 1m 10s\tremaining: 19.8s\n",
      "390:\ttotal: 1m 10s\tremaining: 19.6s\n",
      "391:\ttotal: 1m 10s\tremaining: 19.4s\n",
      "392:\ttotal: 1m 10s\tremaining: 19.2s\n",
      "393:\ttotal: 1m 10s\tremaining: 19s\n",
      "394:\ttotal: 1m 10s\tremaining: 18.8s\n",
      "395:\ttotal: 1m 11s\tremaining: 18.7s\n",
      "396:\ttotal: 1m 11s\tremaining: 18.5s\n",
      "397:\ttotal: 1m 11s\tremaining: 18.3s\n",
      "398:\ttotal: 1m 11s\tremaining: 18.1s\n",
      "399:\ttotal: 1m 11s\tremaining: 17.9s\n",
      "400:\ttotal: 1m 11s\tremaining: 17.8s\n",
      "401:\ttotal: 1m 12s\tremaining: 17.6s\n",
      "402:\ttotal: 1m 12s\tremaining: 17.4s\n",
      "403:\ttotal: 1m 12s\tremaining: 17.2s\n",
      "404:\ttotal: 1m 12s\tremaining: 17s\n",
      "405:\ttotal: 1m 12s\tremaining: 16.9s\n",
      "406:\ttotal: 1m 12s\tremaining: 16.7s\n",
      "407:\ttotal: 1m 13s\tremaining: 16.5s\n",
      "408:\ttotal: 1m 13s\tremaining: 16.3s\n",
      "409:\ttotal: 1m 13s\tremaining: 16.1s\n",
      "410:\ttotal: 1m 13s\tremaining: 16s\n",
      "411:\ttotal: 1m 13s\tremaining: 15.8s\n",
      "412:\ttotal: 1m 14s\tremaining: 15.6s\n",
      "413:\ttotal: 1m 14s\tremaining: 15.4s\n",
      "414:\ttotal: 1m 14s\tremaining: 15.2s\n",
      "415:\ttotal: 1m 14s\tremaining: 15.1s\n",
      "416:\ttotal: 1m 14s\tremaining: 14.9s\n",
      "417:\ttotal: 1m 14s\tremaining: 14.7s\n",
      "418:\ttotal: 1m 15s\tremaining: 14.5s\n",
      "419:\ttotal: 1m 15s\tremaining: 14.3s\n",
      "420:\ttotal: 1m 15s\tremaining: 14.1s\n",
      "421:\ttotal: 1m 15s\tremaining: 14s\n",
      "422:\ttotal: 1m 15s\tremaining: 13.8s\n",
      "423:\ttotal: 1m 15s\tremaining: 13.6s\n",
      "424:\ttotal: 1m 16s\tremaining: 13.4s\n",
      "425:\ttotal: 1m 16s\tremaining: 13.2s\n",
      "426:\ttotal: 1m 16s\tremaining: 13.1s\n",
      "427:\ttotal: 1m 16s\tremaining: 12.9s\n",
      "428:\ttotal: 1m 16s\tremaining: 12.7s\n",
      "429:\ttotal: 1m 16s\tremaining: 12.5s\n",
      "430:\ttotal: 1m 17s\tremaining: 12.3s\n",
      "431:\ttotal: 1m 17s\tremaining: 12.2s\n",
      "432:\ttotal: 1m 17s\tremaining: 12s\n",
      "433:\ttotal: 1m 17s\tremaining: 11.8s\n",
      "434:\ttotal: 1m 17s\tremaining: 11.6s\n",
      "435:\ttotal: 1m 17s\tremaining: 11.4s\n",
      "436:\ttotal: 1m 18s\tremaining: 11.2s\n",
      "437:\ttotal: 1m 18s\tremaining: 11.1s\n",
      "438:\ttotal: 1m 18s\tremaining: 10.9s\n",
      "439:\ttotal: 1m 18s\tremaining: 10.7s\n",
      "440:\ttotal: 1m 18s\tremaining: 10.5s\n",
      "441:\ttotal: 1m 18s\tremaining: 10.4s\n",
      "442:\ttotal: 1m 19s\tremaining: 10.2s\n",
      "443:\ttotal: 1m 19s\tremaining: 10s\n",
      "444:\ttotal: 1m 19s\tremaining: 9.84s\n",
      "445:\ttotal: 1m 19s\tremaining: 9.66s\n",
      "446:\ttotal: 1m 19s\tremaining: 9.48s\n",
      "447:\ttotal: 1m 20s\tremaining: 9.3s\n",
      "448:\ttotal: 1m 20s\tremaining: 9.13s\n",
      "449:\ttotal: 1m 20s\tremaining: 8.95s\n",
      "450:\ttotal: 1m 20s\tremaining: 8.76s\n",
      "451:\ttotal: 1m 20s\tremaining: 8.58s\n",
      "452:\ttotal: 1m 20s\tremaining: 8.4s\n",
      "453:\ttotal: 1m 21s\tremaining: 8.22s\n",
      "454:\ttotal: 1m 21s\tremaining: 8.04s\n",
      "455:\ttotal: 1m 21s\tremaining: 7.86s\n",
      "456:\ttotal: 1m 21s\tremaining: 7.68s\n",
      "457:\ttotal: 1m 21s\tremaining: 7.5s\n",
      "458:\ttotal: 1m 21s\tremaining: 7.32s\n",
      "459:\ttotal: 1m 22s\tremaining: 7.14s\n",
      "460:\ttotal: 1m 22s\tremaining: 6.96s\n",
      "461:\ttotal: 1m 22s\tremaining: 6.78s\n",
      "462:\ttotal: 1m 22s\tremaining: 6.6s\n",
      "463:\ttotal: 1m 22s\tremaining: 6.42s\n",
      "464:\ttotal: 1m 22s\tremaining: 6.24s\n",
      "465:\ttotal: 1m 23s\tremaining: 6.06s\n",
      "466:\ttotal: 1m 23s\tremaining: 5.88s\n",
      "467:\ttotal: 1m 23s\tremaining: 5.7s\n",
      "468:\ttotal: 1m 23s\tremaining: 5.52s\n",
      "469:\ttotal: 1m 23s\tremaining: 5.35s\n",
      "470:\ttotal: 1m 23s\tremaining: 5.17s\n",
      "471:\ttotal: 1m 24s\tremaining: 4.99s\n",
      "472:\ttotal: 1m 24s\tremaining: 4.81s\n",
      "473:\ttotal: 1m 24s\tremaining: 4.64s\n",
      "474:\ttotal: 1m 24s\tremaining: 4.46s\n",
      "475:\ttotal: 1m 24s\tremaining: 4.28s\n",
      "476:\ttotal: 1m 25s\tremaining: 4.1s\n",
      "477:\ttotal: 1m 25s\tremaining: 3.92s\n",
      "478:\ttotal: 1m 25s\tremaining: 3.74s\n",
      "479:\ttotal: 1m 25s\tremaining: 3.56s\n",
      "480:\ttotal: 1m 25s\tremaining: 3.38s\n",
      "481:\ttotal: 1m 25s\tremaining: 3.21s\n",
      "482:\ttotal: 1m 26s\tremaining: 3.03s\n",
      "483:\ttotal: 1m 26s\tremaining: 2.85s\n",
      "484:\ttotal: 1m 26s\tremaining: 2.67s\n",
      "485:\ttotal: 1m 26s\tremaining: 2.49s\n",
      "486:\ttotal: 1m 26s\tremaining: 2.31s\n",
      "487:\ttotal: 1m 26s\tremaining: 2.14s\n",
      "488:\ttotal: 1m 27s\tremaining: 1.96s\n",
      "489:\ttotal: 1m 27s\tremaining: 1.78s\n",
      "490:\ttotal: 1m 27s\tremaining: 1.6s\n",
      "491:\ttotal: 1m 27s\tremaining: 1.42s\n",
      "492:\ttotal: 1m 27s\tremaining: 1.25s\n",
      "493:\ttotal: 1m 27s\tremaining: 1.07s\n",
      "494:\ttotal: 1m 28s\tremaining: 889ms\n",
      "495:\ttotal: 1m 28s\tremaining: 711ms\n",
      "496:\ttotal: 1m 28s\tremaining: 533ms\n",
      "497:\ttotal: 1m 28s\tremaining: 356ms\n",
      "498:\ttotal: 1m 28s\tremaining: 178ms\n",
      "499:\ttotal: 1m 28s\tremaining: 0us\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x16238a20370>"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "auc_cbc1 = auc_from_proba(clf1, train, test)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# With Categorical features\r\n",
    "cat_features_index = [0, 1, 2, 3, 4, 5, 6]\r\n",
    "\r\n",
    "clf2 = cb.CatBoostClassifier(\r\n",
    "    eval_metric=\"AUC\",\r\n",
    "    one_hot_max_size=31,\r\n",
    "    depth=10,\r\n",
    "    iterations=500,\r\n",
    "    l2_leaf_reg=9,\r\n",
    "    learning_rate=0.15,\r\n",
    ")\r\n",
    "clf2.fit(train, y_train, cat_features=cat_features_index)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:\ttotal: 911ms\tremaining: 7m 34s\n",
      "1:\ttotal: 1.78s\tremaining: 7m 24s\n",
      "2:\ttotal: 2.65s\tremaining: 7m 18s\n",
      "3:\ttotal: 3.33s\tremaining: 6m 52s\n",
      "4:\ttotal: 4s\tremaining: 6m 36s\n",
      "5:\ttotal: 4.75s\tremaining: 6m 31s\n",
      "6:\ttotal: 5.37s\tremaining: 6m 18s\n",
      "7:\ttotal: 6.12s\tremaining: 6m 16s\n",
      "8:\ttotal: 6.68s\tremaining: 6m 4s\n",
      "9:\ttotal: 7.25s\tremaining: 5m 55s\n",
      "10:\ttotal: 7.78s\tremaining: 5m 46s\n",
      "11:\ttotal: 8.42s\tremaining: 5m 42s\n",
      "12:\ttotal: 9.12s\tremaining: 5m 41s\n",
      "13:\ttotal: 9.65s\tremaining: 5m 34s\n",
      "14:\ttotal: 10.3s\tremaining: 5m 32s\n",
      "15:\ttotal: 10.9s\tremaining: 5m 30s\n",
      "16:\ttotal: 11.6s\tremaining: 5m 28s\n",
      "17:\ttotal: 12.1s\tremaining: 5m 24s\n",
      "18:\ttotal: 12.9s\tremaining: 5m 25s\n",
      "19:\ttotal: 13.5s\tremaining: 5m 24s\n",
      "20:\ttotal: 14.2s\tremaining: 5m 23s\n",
      "21:\ttotal: 15s\tremaining: 5m 26s\n",
      "22:\ttotal: 15.7s\tremaining: 5m 25s\n",
      "23:\ttotal: 16.4s\tremaining: 5m 25s\n",
      "24:\ttotal: 17s\tremaining: 5m 23s\n",
      "25:\ttotal: 17.7s\tremaining: 5m 23s\n",
      "26:\ttotal: 18.4s\tremaining: 5m 22s\n",
      "27:\ttotal: 19.3s\tremaining: 5m 25s\n",
      "28:\ttotal: 20.2s\tremaining: 5m 28s\n",
      "29:\ttotal: 20.8s\tremaining: 5m 25s\n",
      "30:\ttotal: 21.5s\tremaining: 5m 24s\n",
      "31:\ttotal: 22.2s\tremaining: 5m 24s\n",
      "32:\ttotal: 22.8s\tremaining: 5m 22s\n",
      "33:\ttotal: 23.4s\tremaining: 5m 20s\n",
      "34:\ttotal: 24s\tremaining: 5m 18s\n",
      "35:\ttotal: 24.5s\tremaining: 5m 15s\n",
      "36:\ttotal: 25.1s\tremaining: 5m 14s\n",
      "37:\ttotal: 25.7s\tremaining: 5m 12s\n",
      "38:\ttotal: 26.3s\tremaining: 5m 11s\n",
      "39:\ttotal: 27s\tremaining: 5m 10s\n",
      "40:\ttotal: 27.6s\tremaining: 5m 8s\n",
      "41:\ttotal: 28.3s\tremaining: 5m 8s\n",
      "42:\ttotal: 28.9s\tremaining: 5m 7s\n",
      "43:\ttotal: 29.6s\tremaining: 5m 6s\n",
      "44:\ttotal: 30.4s\tremaining: 5m 7s\n",
      "45:\ttotal: 31.1s\tremaining: 5m 6s\n",
      "46:\ttotal: 31.7s\tremaining: 5m 5s\n",
      "47:\ttotal: 32.5s\tremaining: 5m 5s\n",
      "48:\ttotal: 33.1s\tremaining: 5m 4s\n",
      "49:\ttotal: 33.6s\tremaining: 5m 2s\n",
      "50:\ttotal: 34.2s\tremaining: 5m\n",
      "51:\ttotal: 34.8s\tremaining: 4m 59s\n",
      "52:\ttotal: 35.4s\tremaining: 4m 58s\n",
      "53:\ttotal: 36.1s\tremaining: 4m 57s\n",
      "54:\ttotal: 36.9s\tremaining: 4m 58s\n",
      "55:\ttotal: 37.5s\tremaining: 4m 57s\n",
      "56:\ttotal: 38.1s\tremaining: 4m 56s\n",
      "57:\ttotal: 38.8s\tremaining: 4m 55s\n",
      "58:\ttotal: 39.4s\tremaining: 4m 54s\n",
      "59:\ttotal: 40.1s\tremaining: 4m 53s\n",
      "60:\ttotal: 40.8s\tremaining: 4m 53s\n",
      "61:\ttotal: 41.4s\tremaining: 4m 52s\n",
      "62:\ttotal: 42.1s\tremaining: 4m 51s\n",
      "63:\ttotal: 42.6s\tremaining: 4m 50s\n",
      "64:\ttotal: 43.2s\tremaining: 4m 49s\n",
      "65:\ttotal: 43.9s\tremaining: 4m 48s\n",
      "66:\ttotal: 44.5s\tremaining: 4m 47s\n",
      "67:\ttotal: 45.1s\tremaining: 4m 46s\n",
      "68:\ttotal: 45.7s\tremaining: 4m 45s\n",
      "69:\ttotal: 46.2s\tremaining: 4m 43s\n",
      "70:\ttotal: 46.9s\tremaining: 4m 43s\n",
      "71:\ttotal: 47.8s\tremaining: 4m 44s\n",
      "72:\ttotal: 48.5s\tremaining: 4m 43s\n",
      "73:\ttotal: 49.1s\tremaining: 4m 42s\n",
      "74:\ttotal: 49.7s\tremaining: 4m 41s\n",
      "75:\ttotal: 50.3s\tremaining: 4m 40s\n",
      "76:\ttotal: 51s\tremaining: 4m 40s\n",
      "77:\ttotal: 51.7s\tremaining: 4m 39s\n",
      "78:\ttotal: 52.3s\tremaining: 4m 38s\n",
      "79:\ttotal: 53s\tremaining: 4m 38s\n",
      "80:\ttotal: 53.9s\tremaining: 4m 38s\n",
      "81:\ttotal: 54.5s\tremaining: 4m 37s\n",
      "82:\ttotal: 55s\tremaining: 4m 36s\n",
      "83:\ttotal: 55.7s\tremaining: 4m 35s\n",
      "84:\ttotal: 56.6s\tremaining: 4m 36s\n",
      "85:\ttotal: 57.3s\tremaining: 4m 35s\n",
      "86:\ttotal: 57.9s\tremaining: 4m 34s\n",
      "87:\ttotal: 58.7s\tremaining: 4m 34s\n",
      "88:\ttotal: 59.4s\tremaining: 4m 34s\n",
      "89:\ttotal: 1m\tremaining: 4m 33s\n",
      "90:\ttotal: 1m\tremaining: 4m 32s\n",
      "91:\ttotal: 1m 1s\tremaining: 4m 31s\n",
      "92:\ttotal: 1m 1s\tremaining: 4m 30s\n",
      "93:\ttotal: 1m 2s\tremaining: 4m 29s\n",
      "94:\ttotal: 1m 3s\tremaining: 4m 28s\n",
      "95:\ttotal: 1m 3s\tremaining: 4m 28s\n",
      "96:\ttotal: 1m 4s\tremaining: 4m 26s\n",
      "97:\ttotal: 1m 4s\tremaining: 4m 25s\n",
      "98:\ttotal: 1m 5s\tremaining: 4m 25s\n",
      "99:\ttotal: 1m 6s\tremaining: 4m 24s\n",
      "100:\ttotal: 1m 6s\tremaining: 4m 23s\n",
      "101:\ttotal: 1m 7s\tremaining: 4m 22s\n",
      "102:\ttotal: 1m 7s\tremaining: 4m 21s\n",
      "103:\ttotal: 1m 8s\tremaining: 4m 20s\n",
      "104:\ttotal: 1m 9s\tremaining: 4m 20s\n",
      "105:\ttotal: 1m 9s\tremaining: 4m 19s\n",
      "106:\ttotal: 1m 10s\tremaining: 4m 18s\n",
      "107:\ttotal: 1m 10s\tremaining: 4m 17s\n",
      "108:\ttotal: 1m 11s\tremaining: 4m 16s\n",
      "109:\ttotal: 1m 12s\tremaining: 4m 16s\n",
      "110:\ttotal: 1m 12s\tremaining: 4m 15s\n",
      "111:\ttotal: 1m 13s\tremaining: 4m 14s\n",
      "112:\ttotal: 1m 14s\tremaining: 4m 14s\n",
      "113:\ttotal: 1m 14s\tremaining: 4m 13s\n",
      "114:\ttotal: 1m 15s\tremaining: 4m 12s\n",
      "115:\ttotal: 1m 16s\tremaining: 4m 11s\n",
      "116:\ttotal: 1m 16s\tremaining: 4m 11s\n",
      "117:\ttotal: 1m 17s\tremaining: 4m 10s\n",
      "118:\ttotal: 1m 18s\tremaining: 4m 9s\n",
      "119:\ttotal: 1m 18s\tremaining: 4m 8s\n",
      "120:\ttotal: 1m 19s\tremaining: 4m 8s\n",
      "121:\ttotal: 1m 19s\tremaining: 4m 7s\n",
      "122:\ttotal: 1m 20s\tremaining: 4m 7s\n",
      "123:\ttotal: 1m 21s\tremaining: 4m 6s\n",
      "124:\ttotal: 1m 21s\tremaining: 4m 5s\n",
      "125:\ttotal: 1m 22s\tremaining: 4m 4s\n",
      "126:\ttotal: 1m 23s\tremaining: 4m 3s\n",
      "127:\ttotal: 1m 23s\tremaining: 4m 3s\n",
      "128:\ttotal: 1m 24s\tremaining: 4m 2s\n",
      "129:\ttotal: 1m 24s\tremaining: 4m 1s\n",
      "130:\ttotal: 1m 25s\tremaining: 4m\n",
      "131:\ttotal: 1m 26s\tremaining: 3m 59s\n",
      "132:\ttotal: 1m 26s\tremaining: 3m 59s\n",
      "133:\ttotal: 1m 27s\tremaining: 3m 58s\n",
      "134:\ttotal: 1m 27s\tremaining: 3m 57s\n",
      "135:\ttotal: 1m 28s\tremaining: 3m 56s\n",
      "136:\ttotal: 1m 28s\tremaining: 3m 55s\n",
      "137:\ttotal: 1m 29s\tremaining: 3m 54s\n",
      "138:\ttotal: 1m 29s\tremaining: 3m 53s\n",
      "139:\ttotal: 1m 30s\tremaining: 3m 52s\n",
      "140:\ttotal: 1m 31s\tremaining: 3m 51s\n",
      "141:\ttotal: 1m 31s\tremaining: 3m 51s\n",
      "142:\ttotal: 1m 32s\tremaining: 3m 50s\n",
      "143:\ttotal: 1m 32s\tremaining: 3m 49s\n",
      "144:\ttotal: 1m 33s\tremaining: 3m 48s\n",
      "145:\ttotal: 1m 34s\tremaining: 3m 48s\n",
      "146:\ttotal: 1m 34s\tremaining: 3m 47s\n",
      "147:\ttotal: 1m 35s\tremaining: 3m 46s\n",
      "148:\ttotal: 1m 35s\tremaining: 3m 46s\n",
      "149:\ttotal: 1m 36s\tremaining: 3m 45s\n",
      "150:\ttotal: 1m 37s\tremaining: 3m 44s\n",
      "151:\ttotal: 1m 37s\tremaining: 3m 44s\n",
      "152:\ttotal: 1m 38s\tremaining: 3m 43s\n",
      "153:\ttotal: 1m 39s\tremaining: 3m 42s\n",
      "154:\ttotal: 1m 39s\tremaining: 3m 42s\n",
      "155:\ttotal: 1m 40s\tremaining: 3m 41s\n",
      "156:\ttotal: 1m 40s\tremaining: 3m 40s\n",
      "157:\ttotal: 1m 41s\tremaining: 3m 39s\n",
      "158:\ttotal: 1m 42s\tremaining: 3m 39s\n",
      "159:\ttotal: 1m 42s\tremaining: 3m 38s\n",
      "160:\ttotal: 1m 43s\tremaining: 3m 37s\n",
      "161:\ttotal: 1m 43s\tremaining: 3m 36s\n",
      "162:\ttotal: 1m 44s\tremaining: 3m 36s\n",
      "163:\ttotal: 1m 45s\tremaining: 3m 35s\n",
      "164:\ttotal: 1m 45s\tremaining: 3m 34s\n",
      "165:\ttotal: 1m 46s\tremaining: 3m 33s\n",
      "166:\ttotal: 1m 46s\tremaining: 3m 33s\n",
      "167:\ttotal: 1m 47s\tremaining: 3m 32s\n",
      "168:\ttotal: 1m 48s\tremaining: 3m 31s\n",
      "169:\ttotal: 1m 48s\tremaining: 3m 30s\n",
      "170:\ttotal: 1m 49s\tremaining: 3m 30s\n",
      "171:\ttotal: 1m 49s\tremaining: 3m 29s\n",
      "172:\ttotal: 1m 50s\tremaining: 3m 28s\n",
      "173:\ttotal: 1m 51s\tremaining: 3m 28s\n",
      "174:\ttotal: 1m 51s\tremaining: 3m 27s\n",
      "175:\ttotal: 1m 52s\tremaining: 3m 26s\n",
      "176:\ttotal: 1m 52s\tremaining: 3m 25s\n",
      "177:\ttotal: 1m 53s\tremaining: 3m 25s\n",
      "178:\ttotal: 1m 54s\tremaining: 3m 24s\n",
      "179:\ttotal: 1m 54s\tremaining: 3m 23s\n",
      "180:\ttotal: 1m 55s\tremaining: 3m 23s\n",
      "181:\ttotal: 1m 55s\tremaining: 3m 22s\n",
      "182:\ttotal: 1m 56s\tremaining: 3m 21s\n",
      "183:\ttotal: 1m 57s\tremaining: 3m 21s\n",
      "184:\ttotal: 1m 57s\tremaining: 3m 20s\n",
      "185:\ttotal: 1m 58s\tremaining: 3m 20s\n",
      "186:\ttotal: 1m 59s\tremaining: 3m 19s\n",
      "187:\ttotal: 1m 59s\tremaining: 3m 18s\n",
      "188:\ttotal: 2m\tremaining: 3m 18s\n",
      "189:\ttotal: 2m 1s\tremaining: 3m 17s\n",
      "190:\ttotal: 2m 1s\tremaining: 3m 17s\n",
      "191:\ttotal: 2m 2s\tremaining: 3m 16s\n",
      "192:\ttotal: 2m 3s\tremaining: 3m 15s\n",
      "193:\ttotal: 2m 3s\tremaining: 3m 15s\n",
      "194:\ttotal: 2m 4s\tremaining: 3m 14s\n",
      "195:\ttotal: 2m 5s\tremaining: 3m 13s\n",
      "196:\ttotal: 2m 5s\tremaining: 3m 13s\n",
      "197:\ttotal: 2m 6s\tremaining: 3m 12s\n",
      "198:\ttotal: 2m 6s\tremaining: 3m 11s\n",
      "199:\ttotal: 2m 7s\tremaining: 3m 11s\n",
      "200:\ttotal: 2m 8s\tremaining: 3m 10s\n",
      "201:\ttotal: 2m 8s\tremaining: 3m 9s\n",
      "202:\ttotal: 2m 9s\tremaining: 3m 9s\n",
      "203:\ttotal: 2m 9s\tremaining: 3m 8s\n",
      "204:\ttotal: 2m 10s\tremaining: 3m 7s\n",
      "205:\ttotal: 2m 11s\tremaining: 3m 7s\n",
      "206:\ttotal: 2m 11s\tremaining: 3m 6s\n",
      "207:\ttotal: 2m 12s\tremaining: 3m 5s\n",
      "208:\ttotal: 2m 12s\tremaining: 3m 5s\n",
      "209:\ttotal: 2m 13s\tremaining: 3m 4s\n",
      "210:\ttotal: 2m 14s\tremaining: 3m 3s\n",
      "211:\ttotal: 2m 14s\tremaining: 3m 3s\n",
      "212:\ttotal: 2m 15s\tremaining: 3m 2s\n",
      "213:\ttotal: 2m 15s\tremaining: 3m 1s\n",
      "214:\ttotal: 2m 16s\tremaining: 3m\n",
      "215:\ttotal: 2m 17s\tremaining: 3m\n",
      "216:\ttotal: 2m 17s\tremaining: 2m 59s\n",
      "217:\ttotal: 2m 18s\tremaining: 2m 58s\n",
      "218:\ttotal: 2m 18s\tremaining: 2m 58s\n",
      "219:\ttotal: 2m 19s\tremaining: 2m 57s\n",
      "220:\ttotal: 2m 20s\tremaining: 2m 57s\n",
      "221:\ttotal: 2m 20s\tremaining: 2m 56s\n",
      "222:\ttotal: 2m 21s\tremaining: 2m 55s\n",
      "223:\ttotal: 2m 22s\tremaining: 2m 55s\n",
      "224:\ttotal: 2m 22s\tremaining: 2m 54s\n",
      "225:\ttotal: 2m 23s\tremaining: 2m 54s\n",
      "226:\ttotal: 2m 24s\tremaining: 2m 53s\n",
      "227:\ttotal: 2m 25s\tremaining: 2m 53s\n",
      "228:\ttotal: 2m 25s\tremaining: 2m 52s\n",
      "229:\ttotal: 2m 26s\tremaining: 2m 52s\n",
      "230:\ttotal: 2m 27s\tremaining: 2m 51s\n",
      "231:\ttotal: 2m 27s\tremaining: 2m 50s\n",
      "232:\ttotal: 2m 28s\tremaining: 2m 49s\n",
      "233:\ttotal: 2m 28s\tremaining: 2m 49s\n",
      "234:\ttotal: 2m 29s\tremaining: 2m 48s\n",
      "235:\ttotal: 2m 30s\tremaining: 2m 48s\n",
      "236:\ttotal: 2m 31s\tremaining: 2m 47s\n",
      "237:\ttotal: 2m 31s\tremaining: 2m 47s\n",
      "238:\ttotal: 2m 32s\tremaining: 2m 46s\n",
      "239:\ttotal: 2m 33s\tremaining: 2m 46s\n",
      "240:\ttotal: 2m 33s\tremaining: 2m 45s\n",
      "241:\ttotal: 2m 34s\tremaining: 2m 44s\n",
      "242:\ttotal: 2m 35s\tremaining: 2m 44s\n",
      "243:\ttotal: 2m 35s\tremaining: 2m 43s\n",
      "244:\ttotal: 2m 36s\tremaining: 2m 42s\n",
      "245:\ttotal: 2m 37s\tremaining: 2m 42s\n",
      "246:\ttotal: 2m 37s\tremaining: 2m 41s\n",
      "247:\ttotal: 2m 38s\tremaining: 2m 40s\n",
      "248:\ttotal: 2m 38s\tremaining: 2m 40s\n",
      "249:\ttotal: 2m 39s\tremaining: 2m 39s\n",
      "250:\ttotal: 2m 39s\tremaining: 2m 38s\n",
      "251:\ttotal: 2m 40s\tremaining: 2m 37s\n",
      "252:\ttotal: 2m 40s\tremaining: 2m 37s\n",
      "253:\ttotal: 2m 41s\tremaining: 2m 36s\n",
      "254:\ttotal: 2m 41s\tremaining: 2m 35s\n",
      "255:\ttotal: 2m 42s\tremaining: 2m 34s\n",
      "256:\ttotal: 2m 43s\tremaining: 2m 34s\n",
      "257:\ttotal: 2m 43s\tremaining: 2m 33s\n",
      "258:\ttotal: 2m 44s\tremaining: 2m 32s\n",
      "259:\ttotal: 2m 44s\tremaining: 2m 32s\n",
      "260:\ttotal: 2m 45s\tremaining: 2m 31s\n",
      "261:\ttotal: 2m 45s\tremaining: 2m 30s\n",
      "262:\ttotal: 2m 46s\tremaining: 2m 30s\n",
      "263:\ttotal: 2m 47s\tremaining: 2m 29s\n",
      "264:\ttotal: 2m 47s\tremaining: 2m 28s\n",
      "265:\ttotal: 2m 48s\tremaining: 2m 28s\n",
      "266:\ttotal: 2m 48s\tremaining: 2m 27s\n",
      "267:\ttotal: 2m 49s\tremaining: 2m 26s\n",
      "268:\ttotal: 2m 49s\tremaining: 2m 25s\n",
      "269:\ttotal: 2m 50s\tremaining: 2m 25s\n",
      "270:\ttotal: 2m 51s\tremaining: 2m 24s\n",
      "271:\ttotal: 2m 51s\tremaining: 2m 24s\n",
      "272:\ttotal: 2m 52s\tremaining: 2m 23s\n",
      "273:\ttotal: 2m 52s\tremaining: 2m 22s\n",
      "274:\ttotal: 2m 53s\tremaining: 2m 22s\n",
      "275:\ttotal: 2m 54s\tremaining: 2m 21s\n",
      "276:\ttotal: 2m 55s\tremaining: 2m 21s\n",
      "277:\ttotal: 2m 55s\tremaining: 2m 20s\n",
      "278:\ttotal: 2m 56s\tremaining: 2m 20s\n",
      "279:\ttotal: 2m 57s\tremaining: 2m 19s\n",
      "280:\ttotal: 2m 58s\tremaining: 2m 19s\n",
      "281:\ttotal: 2m 59s\tremaining: 2m 18s\n",
      "282:\ttotal: 2m 59s\tremaining: 2m 17s\n",
      "283:\ttotal: 3m\tremaining: 2m 17s\n",
      "284:\ttotal: 3m 1s\tremaining: 2m 16s\n",
      "285:\ttotal: 3m 1s\tremaining: 2m 15s\n",
      "286:\ttotal: 3m 2s\tremaining: 2m 15s\n",
      "287:\ttotal: 3m 3s\tremaining: 2m 14s\n",
      "288:\ttotal: 3m 3s\tremaining: 2m 14s\n",
      "289:\ttotal: 3m 4s\tremaining: 2m 13s\n",
      "290:\ttotal: 3m 4s\tremaining: 2m 12s\n",
      "291:\ttotal: 3m 5s\tremaining: 2m 12s\n",
      "292:\ttotal: 3m 6s\tremaining: 2m 11s\n",
      "293:\ttotal: 3m 6s\tremaining: 2m 10s\n",
      "294:\ttotal: 3m 7s\tremaining: 2m 10s\n",
      "295:\ttotal: 3m 7s\tremaining: 2m 9s\n",
      "296:\ttotal: 3m 8s\tremaining: 2m 8s\n",
      "297:\ttotal: 3m 9s\tremaining: 2m 8s\n",
      "298:\ttotal: 3m 9s\tremaining: 2m 7s\n",
      "299:\ttotal: 3m 10s\tremaining: 2m 6s\n",
      "300:\ttotal: 3m 10s\tremaining: 2m 6s\n",
      "301:\ttotal: 3m 11s\tremaining: 2m 5s\n",
      "302:\ttotal: 3m 12s\tremaining: 2m 4s\n",
      "303:\ttotal: 3m 12s\tremaining: 2m 4s\n",
      "304:\ttotal: 3m 13s\tremaining: 2m 3s\n",
      "305:\ttotal: 3m 13s\tremaining: 2m 2s\n",
      "306:\ttotal: 3m 14s\tremaining: 2m 2s\n",
      "307:\ttotal: 3m 14s\tremaining: 2m 1s\n",
      "308:\ttotal: 3m 15s\tremaining: 2m\n",
      "309:\ttotal: 3m 16s\tremaining: 2m\n",
      "310:\ttotal: 3m 16s\tremaining: 1m 59s\n",
      "311:\ttotal: 3m 17s\tremaining: 1m 58s\n",
      "312:\ttotal: 3m 17s\tremaining: 1m 58s\n",
      "313:\ttotal: 3m 18s\tremaining: 1m 57s\n",
      "314:\ttotal: 3m 18s\tremaining: 1m 56s\n",
      "315:\ttotal: 3m 19s\tremaining: 1m 56s\n",
      "316:\ttotal: 3m 20s\tremaining: 1m 55s\n",
      "317:\ttotal: 3m 20s\tremaining: 1m 54s\n",
      "318:\ttotal: 3m 21s\tremaining: 1m 54s\n",
      "319:\ttotal: 3m 21s\tremaining: 1m 53s\n",
      "320:\ttotal: 3m 22s\tremaining: 1m 52s\n",
      "321:\ttotal: 3m 22s\tremaining: 1m 52s\n",
      "322:\ttotal: 3m 23s\tremaining: 1m 51s\n",
      "323:\ttotal: 3m 24s\tremaining: 1m 50s\n",
      "324:\ttotal: 3m 24s\tremaining: 1m 50s\n",
      "325:\ttotal: 3m 25s\tremaining: 1m 49s\n",
      "326:\ttotal: 3m 25s\tremaining: 1m 48s\n",
      "327:\ttotal: 3m 26s\tremaining: 1m 48s\n",
      "328:\ttotal: 3m 26s\tremaining: 1m 47s\n",
      "329:\ttotal: 3m 27s\tremaining: 1m 46s\n",
      "330:\ttotal: 3m 28s\tremaining: 1m 46s\n",
      "331:\ttotal: 3m 28s\tremaining: 1m 45s\n",
      "332:\ttotal: 3m 29s\tremaining: 1m 44s\n",
      "333:\ttotal: 3m 30s\tremaining: 1m 44s\n",
      "334:\ttotal: 3m 30s\tremaining: 1m 43s\n",
      "335:\ttotal: 3m 31s\tremaining: 1m 43s\n",
      "336:\ttotal: 3m 31s\tremaining: 1m 42s\n",
      "337:\ttotal: 3m 32s\tremaining: 1m 41s\n",
      "338:\ttotal: 3m 32s\tremaining: 1m 41s\n",
      "339:\ttotal: 3m 33s\tremaining: 1m 40s\n",
      "340:\ttotal: 3m 33s\tremaining: 1m 39s\n",
      "341:\ttotal: 3m 34s\tremaining: 1m 39s\n",
      "342:\ttotal: 3m 35s\tremaining: 1m 38s\n",
      "343:\ttotal: 3m 35s\tremaining: 1m 37s\n",
      "344:\ttotal: 3m 36s\tremaining: 1m 37s\n",
      "345:\ttotal: 3m 36s\tremaining: 1m 36s\n",
      "346:\ttotal: 3m 37s\tremaining: 1m 35s\n",
      "347:\ttotal: 3m 38s\tremaining: 1m 35s\n",
      "348:\ttotal: 3m 38s\tremaining: 1m 34s\n",
      "349:\ttotal: 3m 39s\tremaining: 1m 33s\n",
      "350:\ttotal: 3m 39s\tremaining: 1m 33s\n",
      "351:\ttotal: 3m 40s\tremaining: 1m 32s\n",
      "352:\ttotal: 3m 41s\tremaining: 1m 32s\n",
      "353:\ttotal: 3m 41s\tremaining: 1m 31s\n",
      "354:\ttotal: 3m 42s\tremaining: 1m 30s\n",
      "355:\ttotal: 3m 42s\tremaining: 1m 30s\n",
      "356:\ttotal: 3m 43s\tremaining: 1m 29s\n",
      "357:\ttotal: 3m 44s\tremaining: 1m 28s\n",
      "358:\ttotal: 3m 44s\tremaining: 1m 28s\n",
      "359:\ttotal: 3m 45s\tremaining: 1m 27s\n",
      "360:\ttotal: 3m 45s\tremaining: 1m 26s\n",
      "361:\ttotal: 3m 46s\tremaining: 1m 26s\n",
      "362:\ttotal: 3m 46s\tremaining: 1m 25s\n",
      "363:\ttotal: 3m 47s\tremaining: 1m 24s\n",
      "364:\ttotal: 3m 48s\tremaining: 1m 24s\n",
      "365:\ttotal: 3m 48s\tremaining: 1m 23s\n",
      "366:\ttotal: 3m 49s\tremaining: 1m 23s\n",
      "367:\ttotal: 3m 50s\tremaining: 1m 22s\n",
      "368:\ttotal: 3m 51s\tremaining: 1m 22s\n",
      "369:\ttotal: 3m 52s\tremaining: 1m 21s\n",
      "370:\ttotal: 3m 53s\tremaining: 1m 21s\n",
      "371:\ttotal: 3m 53s\tremaining: 1m 20s\n",
      "372:\ttotal: 3m 54s\tremaining: 1m 19s\n",
      "373:\ttotal: 3m 54s\tremaining: 1m 19s\n",
      "374:\ttotal: 3m 55s\tremaining: 1m 18s\n",
      "375:\ttotal: 3m 56s\tremaining: 1m 17s\n",
      "376:\ttotal: 3m 57s\tremaining: 1m 17s\n",
      "377:\ttotal: 3m 57s\tremaining: 1m 16s\n",
      "378:\ttotal: 3m 58s\tremaining: 1m 16s\n",
      "379:\ttotal: 3m 58s\tremaining: 1m 15s\n",
      "380:\ttotal: 3m 59s\tremaining: 1m 14s\n",
      "381:\ttotal: 3m 59s\tremaining: 1m 14s\n",
      "382:\ttotal: 4m\tremaining: 1m 13s\n",
      "383:\ttotal: 4m 1s\tremaining: 1m 12s\n",
      "384:\ttotal: 4m 2s\tremaining: 1m 12s\n",
      "385:\ttotal: 4m 2s\tremaining: 1m 11s\n",
      "386:\ttotal: 4m 3s\tremaining: 1m 11s\n",
      "387:\ttotal: 4m 3s\tremaining: 1m 10s\n",
      "388:\ttotal: 4m 4s\tremaining: 1m 9s\n",
      "389:\ttotal: 4m 5s\tremaining: 1m 9s\n",
      "390:\ttotal: 4m 5s\tremaining: 1m 8s\n",
      "391:\ttotal: 4m 6s\tremaining: 1m 7s\n",
      "392:\ttotal: 4m 6s\tremaining: 1m 7s\n",
      "393:\ttotal: 4m 7s\tremaining: 1m 6s\n",
      "394:\ttotal: 4m 8s\tremaining: 1m 5s\n",
      "395:\ttotal: 4m 8s\tremaining: 1m 5s\n",
      "396:\ttotal: 4m 9s\tremaining: 1m 4s\n",
      "397:\ttotal: 4m 10s\tremaining: 1m 4s\n",
      "398:\ttotal: 4m 11s\tremaining: 1m 3s\n",
      "399:\ttotal: 4m 11s\tremaining: 1m 2s\n",
      "400:\ttotal: 4m 12s\tremaining: 1m 2s\n",
      "401:\ttotal: 4m 13s\tremaining: 1m 1s\n",
      "402:\ttotal: 4m 13s\tremaining: 1m 1s\n",
      "403:\ttotal: 4m 14s\tremaining: 1m\n",
      "404:\ttotal: 4m 14s\tremaining: 59.8s\n",
      "405:\ttotal: 4m 15s\tremaining: 59.2s\n",
      "406:\ttotal: 4m 16s\tremaining: 58.5s\n",
      "407:\ttotal: 4m 16s\tremaining: 57.9s\n",
      "408:\ttotal: 4m 17s\tremaining: 57.3s\n",
      "409:\ttotal: 4m 18s\tremaining: 56.7s\n",
      "410:\ttotal: 4m 18s\tremaining: 56s\n",
      "411:\ttotal: 4m 19s\tremaining: 55.4s\n",
      "412:\ttotal: 4m 19s\tremaining: 54.8s\n",
      "413:\ttotal: 4m 20s\tremaining: 54.1s\n",
      "414:\ttotal: 4m 21s\tremaining: 53.5s\n",
      "415:\ttotal: 4m 21s\tremaining: 52.9s\n",
      "416:\ttotal: 4m 22s\tremaining: 52.3s\n",
      "417:\ttotal: 4m 23s\tremaining: 51.6s\n",
      "418:\ttotal: 4m 23s\tremaining: 51s\n",
      "419:\ttotal: 4m 24s\tremaining: 50.4s\n",
      "420:\ttotal: 4m 25s\tremaining: 49.7s\n",
      "421:\ttotal: 4m 25s\tremaining: 49.1s\n",
      "422:\ttotal: 4m 26s\tremaining: 48.5s\n",
      "423:\ttotal: 4m 26s\tremaining: 47.8s\n",
      "424:\ttotal: 4m 27s\tremaining: 47.2s\n",
      "425:\ttotal: 4m 28s\tremaining: 46.6s\n",
      "426:\ttotal: 4m 28s\tremaining: 45.9s\n",
      "427:\ttotal: 4m 29s\tremaining: 45.3s\n",
      "428:\ttotal: 4m 29s\tremaining: 44.7s\n",
      "429:\ttotal: 4m 30s\tremaining: 44s\n",
      "430:\ttotal: 4m 31s\tremaining: 43.4s\n",
      "431:\ttotal: 4m 31s\tremaining: 42.7s\n",
      "432:\ttotal: 4m 32s\tremaining: 42.1s\n",
      "433:\ttotal: 4m 32s\tremaining: 41.5s\n",
      "434:\ttotal: 4m 33s\tremaining: 40.8s\n",
      "435:\ttotal: 4m 34s\tremaining: 40.2s\n",
      "436:\ttotal: 4m 34s\tremaining: 39.6s\n",
      "437:\ttotal: 4m 35s\tremaining: 38.9s\n",
      "438:\ttotal: 4m 35s\tremaining: 38.3s\n",
      "439:\ttotal: 4m 36s\tremaining: 37.7s\n",
      "440:\ttotal: 4m 36s\tremaining: 37s\n",
      "441:\ttotal: 4m 37s\tremaining: 36.4s\n",
      "442:\ttotal: 4m 38s\tremaining: 35.8s\n",
      "443:\ttotal: 4m 38s\tremaining: 35.2s\n",
      "444:\ttotal: 4m 39s\tremaining: 34.5s\n",
      "445:\ttotal: 4m 40s\tremaining: 33.9s\n",
      "446:\ttotal: 4m 40s\tremaining: 33.3s\n",
      "447:\ttotal: 4m 41s\tremaining: 32.6s\n",
      "448:\ttotal: 4m 41s\tremaining: 32s\n",
      "449:\ttotal: 4m 42s\tremaining: 31.4s\n",
      "450:\ttotal: 4m 43s\tremaining: 30.8s\n",
      "451:\ttotal: 4m 43s\tremaining: 30.1s\n",
      "452:\ttotal: 4m 44s\tremaining: 29.5s\n",
      "453:\ttotal: 4m 44s\tremaining: 28.9s\n",
      "454:\ttotal: 4m 45s\tremaining: 28.2s\n",
      "455:\ttotal: 4m 46s\tremaining: 27.6s\n",
      "456:\ttotal: 4m 46s\tremaining: 27s\n",
      "457:\ttotal: 4m 47s\tremaining: 26.4s\n",
      "458:\ttotal: 4m 48s\tremaining: 25.8s\n",
      "459:\ttotal: 4m 48s\tremaining: 25.1s\n",
      "460:\ttotal: 4m 49s\tremaining: 24.5s\n",
      "461:\ttotal: 4m 50s\tremaining: 23.9s\n",
      "462:\ttotal: 4m 50s\tremaining: 23.2s\n",
      "463:\ttotal: 4m 51s\tremaining: 22.6s\n",
      "464:\ttotal: 4m 52s\tremaining: 22s\n",
      "465:\ttotal: 4m 52s\tremaining: 21.4s\n",
      "466:\ttotal: 4m 53s\tremaining: 20.7s\n",
      "467:\ttotal: 4m 54s\tremaining: 20.1s\n",
      "468:\ttotal: 4m 54s\tremaining: 19.5s\n",
      "469:\ttotal: 4m 55s\tremaining: 18.9s\n",
      "470:\ttotal: 4m 56s\tremaining: 18.2s\n",
      "471:\ttotal: 4m 56s\tremaining: 17.6s\n",
      "472:\ttotal: 4m 57s\tremaining: 17s\n",
      "473:\ttotal: 4m 57s\tremaining: 16.3s\n",
      "474:\ttotal: 4m 58s\tremaining: 15.7s\n",
      "475:\ttotal: 4m 58s\tremaining: 15.1s\n",
      "476:\ttotal: 4m 59s\tremaining: 14.4s\n",
      "477:\ttotal: 5m\tremaining: 13.8s\n",
      "478:\ttotal: 5m\tremaining: 13.2s\n",
      "479:\ttotal: 5m 1s\tremaining: 12.6s\n",
      "480:\ttotal: 5m 2s\tremaining: 11.9s\n",
      "481:\ttotal: 5m 2s\tremaining: 11.3s\n",
      "482:\ttotal: 5m 3s\tremaining: 10.7s\n",
      "483:\ttotal: 5m 3s\tremaining: 10s\n",
      "484:\ttotal: 5m 4s\tremaining: 9.42s\n",
      "485:\ttotal: 5m 5s\tremaining: 8.79s\n",
      "486:\ttotal: 5m 5s\tremaining: 8.16s\n",
      "487:\ttotal: 5m 6s\tremaining: 7.53s\n",
      "488:\ttotal: 5m 6s\tremaining: 6.9s\n",
      "489:\ttotal: 5m 7s\tremaining: 6.28s\n",
      "490:\ttotal: 5m 8s\tremaining: 5.65s\n",
      "491:\ttotal: 5m 9s\tremaining: 5.03s\n",
      "492:\ttotal: 5m 9s\tremaining: 4.4s\n",
      "493:\ttotal: 5m 10s\tremaining: 3.77s\n",
      "494:\ttotal: 5m 11s\tremaining: 3.14s\n",
      "495:\ttotal: 5m 11s\tremaining: 2.52s\n",
      "496:\ttotal: 5m 12s\tremaining: 1.89s\n",
      "497:\ttotal: 5m 13s\tremaining: 1.26s\n",
      "498:\ttotal: 5m 13s\tremaining: 629ms\n",
      "499:\ttotal: 5m 14s\tremaining: 0us\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x16234e4cd60>"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "auc_cbc2 = auc_from_proba(clf2, train, test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "print(np.round(auc_cbc1, 3))\r\n",
    "print(np.round(auc_cbc2, 3))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.834 0.76 ]\n",
      "[0.903 0.832]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Результаты:\r\n",
    "\r\n",
    "<img src='https://miro.medium.com/max/3384/1*w05Hg2QZ5ioDi2OXdCCMiw.png' alignment='center'/>"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Интерпретация результатов из оригинальной статьи\r\n",
    "\r\n",
    "Для оценки модели мы должны изучить производительность модели с точки зрения как скорости, так и точности."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Имея это в виду, CatBoost выходит победителем с максимальной точностью на тестовом наборе (0,816), минимальным переоснащением (точность как train, так и test близка) и минимальным временем прогнозирования и настройки. Но это произошло только потому, что мы рассмотрели категориальные переменные и настроили one_hot_max_size. Если мы не воспользуемся этими особенностями CatBoost, он окажется худшим исполнителем с точностью всего 0,752. Следовательно, мы узнали, что CatBoost работает хорошо только тогда, когда у нас есть категориальные переменные в данных, и мы правильно их настраиваем."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нашим следующим исполнителем был XGBoost, который, как правило, работает хорошо. Его точность была довольно близка к CatBoost даже после игнорирования того факта, что у нас есть категориальные переменные в данных, которые мы преобразовали в числовые значения для его потребления. Однако единственная проблема с XGBoost заключается в том, что он слишком медленный."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Последнее место занимает Light GBM. Здесь важно отметить, что он плохо работал как с точки зрения скорости, так и точности при использовании cat_features. Причина, по которой он плохо работал, заключалась скорее всего в том, что он использует какое—то модифицированное среднее кодирование для категориальных данных, которое вызвало переобучение (точность train довольно высока-0,999 по сравнению с точностью теста). \r\n",
    "Однако, если мы используем его обычно, как XGBoost, он может достичь аналогичной (если не более высокой) точности с гораздо более высокой скоростью по сравнению с XGBoost"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Наконец, важно заметить, что эти наблюдения верны для данного конкретного набора данных и могут оставаться или не оставаться верными для других наборов данных. Однако в целом верно то, что XGBoost работает медленнее, чем два других алгоритма."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GradientBoostingClassifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\r\n",
    "\r\n",
    "model_sk = GradientBoostingClassifier()\r\n",
    "model_sk.fit(train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "auc_sk = auc_from_proba(model_sk, train, test)\r\n",
    "print(np.round(auc_sk, 3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.7   0.696]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Воспроизведенные результаты"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "|                      | XGBoost                                                                     | LightGBM without categorial features                                  | LightGBM with categorial features | CatBoost without categorial features                                                             | CatBoost with categorial features | SKLearn                                            |\r\n",
    "| -------------------- | --------------------------------------------------------------------------- | --------------------------------------------------------------------- | --------------------------------- | ------------------------------------------------------------------------------------------------ | --------------------------------- | -------------------------------------------------- |\r\n",
    "| Params               | max_depth=50,min_child_weight=1,    n_estimators=200,    learning_rate=0.16 |  max_depth=50, learning_rate=0.1, num_leaves=900, n_estimators=300  |                                   | one_hot_max_size=31,    depth=10,    iterations=500,    l2_leaf_reg=9,    learning_rate=0.15                                     || max_depth=10, n_estimators=200, learning_rate=0.16 |\r\n",
    "| Train AUC Score      | 1                                                                           | 0.981                                                                 | 0.998                             | 0.834                                                                                            | 0.903                             |                      0.7                              |\r\n",
    "| Test AUC Score       | 0.789                                                                       | 0.785                                                                 | 0.775                             | 0.76                                                                                             | 0.832                             |                    0.696                                |\r\n",
    "| Training Time, sec   | 505                                                                         | 22                                                                    | 33                                | 89                                                                                               | 332                               | 82                                                 |\r\n",
    "| Prediction Time, sec | 16                                                                          | 10                                                                    | 20                                | 2                                                                                                | 13                                | 2                                                  |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ссылки:\r\n",
    "\r\n",
    "- [Документация sklearn](https://scikit-learn.org/stable/modules/ensemble.html) для реализации ансамблей\r\n",
    "- [AdaBoost](https://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/boosting-survey.pdf)\r\n",
    "- [Модификации AdaBoost](https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf)\r\n",
    "- [Статья про введение в градиентные бустинги](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\r\n",
    "- [XGBoost, LightGBM or CatBoost — which boosting algorithm should I use?](https://medium.com/riskified-technology/xgboost-lightgbm-or-catboost-which-boosting-algorithm-should-i-use-e7fda7bb36bc)\r\n",
    "- [Оригинальная статья про сравнение бустингов](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)\r\n",
    "- [ODS Тема 10. Градиентный бустинг](https://habr.com/ru/company/ods/blog/327250/#2-gbm-algoritm)\r\n",
    "- [Лукция. Машинное обучение. Линейные композиции, бустинг. К.В. Воронцов, ШАД, Яндекс.](https://www.youtube.com/watch?v=5QrWW_ZlP9w&list=PLJOzdkh8T5krxc4HsHbB8g8f0hu7973fK&index=15)\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('classification2021': conda)"
  },
  "interpreter": {
   "hash": "2f95812f1acc88f1aebe4a63ef7278ea42ead32b991dee2e7ab6f3ab22c2607c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}